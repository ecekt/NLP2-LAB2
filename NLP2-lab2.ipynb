{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training sets\n",
    "with open('train.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()]\n",
    "with open('train.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#validation sets\n",
    "with open('val.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_2017_flickr.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_2017_flickr.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 15460\n",
      "Vocabulary size FR 17007\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15460\n",
      "17007\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_corpus2id(sentence_list, tokens2id):\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id = []\n",
    "    \n",
    "    for s in sentence_list:\n",
    "    \n",
    "        sentence2id = []\n",
    "        sentence2id.append(tokens2id['<SOS>'])\n",
    "    \n",
    "        for w in s:\n",
    "            word_id = tokens2id[w]\n",
    "            sentence2id.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id.append(tokens2id['<EOS>'])\n",
    "        corpus2id.append(sentence2id)\n",
    "    \n",
    "    return corpus2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus2id_en = convert_corpus2id(sentence_list_en, tokens2id_en)\n",
    "corpus2id_fr = convert_corpus2id(sentence_list_fr, tokens2id_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2021, 15433, 2129, 8939, 2555, 9954, 9572, 8983, 3797, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2id_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, average loss, duration\n",
      "0 718.2738318443298 718.273831844 0:00:02.183473\n",
      "1 539.3672485351562 628.82054019 0:00:02.072869\n",
      "2 516.5926432609558 591.411241213 0:00:02.061070\n",
      "3 510.5496530532837 571.195844173 0:00:02.071433\n",
      "4 509.6339201927185 558.883459377 0:00:02.044972\n",
      "5 509.70082807540894 550.68635416 0:00:02.038153\n",
      "6 509.78446865081787 544.843227659 0:00:02.050906\n",
      "7 509.79284143447876 540.461929381 0:00:02.067501\n",
      "8 509.7725043296814 537.051993264 0:00:02.035385\n",
      "9 509.7488212585449 534.321676064 0:00:02.051614\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15f05eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "        \n",
    "        self.average_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        \n",
    "        self.attention_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        #do we use non-linearity after attention\n",
    "        \n",
    "        #TODO: DROPOUT\n",
    "        \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "        \n",
    "        ws = []\n",
    "        ps = []\n",
    "        es = []\n",
    "        \n",
    "        for s in range(len(sent_fr)):\n",
    "            word = sent_fr[s]\n",
    "            pos = pos_fr[s]\n",
    "            \n",
    "            w_out = self.w_embeddings(word)\n",
    "\n",
    "            p_out = self.p_embeddings(pos)\n",
    "\n",
    "            e_out = torch.cat((w_out, p_out), 0)\n",
    "    \n",
    "            ws.append(w_out)\n",
    "            ps.append(p_out)\n",
    "            es.append(e_out)\n",
    "        \n",
    "        stacked_contexts = torch.stack(es, dim = 0)\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        average_context = self.average_projection(average_context)\n",
    "        \n",
    "        stacked_contexts = self.attention_projection(stacked_contexts)\n",
    "            \n",
    "        return average_context, stacked_contexts\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "        #self.bidirLSTM = nn.LSTM(self.embedding_dim, self.embedding_dim, bidirectional=True)\n",
    "        #TODO: LSTM, GRU \n",
    "       \n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_avg_context, encoder_stacked_contexts):\n",
    "        \n",
    "        pred = []\n",
    "        attentions = []\n",
    "        \n",
    "        for s in range(len(gold_target_sent)):\n",
    "            gold_word = gold_target_sent[s]\n",
    "            \n",
    "            output = self.embedding(gold_word)\n",
    "\n",
    "            #g_out = self.dropout(g_out)\n",
    "            \n",
    "            if s == 0:\n",
    "            \n",
    "                #TODO: how to add weighted context\n",
    "                #do we set h0 to avg context OR do we input it?\n",
    "                output, hidden = self.rnn(output.view(1, 1, -1), encoder_avg_context.view(1, 1, -1))\n",
    "                prev_hidden = hidden\n",
    "                \n",
    "                s_output = F.softmax(output[0], dim=0) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "                s_output = self.out_affine(s_output)\n",
    "                pred.append(s_output)\n",
    "                \n",
    "            else:\n",
    "                            \n",
    "                #TODO: how to add weighted context\n",
    "                output, hidden = self.rnn(output.view(1, 1, -1), prev_hidden.view(1, 1, -1))\n",
    "                prev_hidden = hidden\n",
    "                \n",
    "                s_output = F.softmax(output[0], dim=0) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "                s_output = self.out_affine(s_output)\n",
    "                pred.append(s_output)\n",
    "                \n",
    "            attention_weights_word = F.softmax(torch.matmul(encoder_stacked_contexts, prev_hidden.view(-1,1)), dim = 0)\n",
    "            attentions.append(attention_weights_word)\n",
    "            \n",
    "        attention_weights = torch.stack(attentions, dim=0)\n",
    "            \n",
    "        pred = torch.stack(pred, dim=0)\n",
    "        return pred, attention_weights #output, hidden \n",
    "\n",
    "    \n",
    "    #https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "w_embedding_dim = 10\n",
    "p_embedding_dim = 10\n",
    "dec_embedding_dim = 10\n",
    "dropout_prob = 0.1\n",
    "\n",
    "model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "\n",
    "optimizer_encoder = optim.Adam(model_encoder.parameters(), lr = learning_rate)\n",
    "optimizer_decoder = optim.Adam(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 100\n",
    "\n",
    "print('epoch, total loss, average loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    " \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "           \n",
    "            sent_fr_pos = torch.tensor(np.asarray(pos_fr), dtype= torch.long)            \n",
    "            sent_en_pos = torch.tensor(np.asarray(pos_en), dtype= torch.long)\n",
    "            \n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "\n",
    "            average_context, stacked_contexts = model_encoder(sent_fr, sent_fr_pos)\n",
    "       \n",
    "            #prev_hidden = average_context or actual prev_hidden\n",
    "        \n",
    "            pred, attention_weights = model_decoder(sent_en, average_context, stacked_contexts)\n",
    "            \n",
    "            loss = loss_func(pred.squeeze(), sent_en)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_encoder.step()\n",
    "            optimizer_decoder.step()\n",
    "\n",
    "            total_loss += loss.item() \n",
    "        \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    print(e, total_loss, avg_loss, now-then)\n",
    "    \n",
    "    avg_losses.append(avg_loss)\n",
    "    \n",
    "\n",
    "with open('model_encoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_encoder,file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_decoder,file)\n",
    "    \n",
    "iteration= list(range(len(avg_losses)))\n",
    "\n",
    "plt.plot(iteration, avg_losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 100\n",
    "\n",
    "with open('model_encoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_encoder = pickle.load(file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_attention(model, sentence):\n",
    "    \n",
    "#************************************************************************\n",
    "# A is the attention torch Tensor: the output of your model\n",
    "# S is the softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "#************************************************************************\n",
    "\n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy() # get the data in Variable, and then the torch Tensor as numpy array\n",
    "    plt.imshow(numpy_S)\n",
    "    plt.savefig(\"attention-sent-{}-epoch-{}\".format(i, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#TEST - INFERENCE\n",
    "#BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(np.asarray([i for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.tensor(np.asarray([i+1 for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "        [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "        [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.]])\n"
     ]
    }
   ],
   "source": [
    "st = torch.stack([a,b,a,b,a], dim = 0)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.],\n",
       "        [  1.,   2.],\n",
       "        [  2.,   3.],\n",
       "        [  3.,   4.],\n",
       "        [  4.,   5.],\n",
       "        [  5.,   6.],\n",
       "        [  6.,   7.],\n",
       "        [  7.,   8.],\n",
       "        [  8.,   9.],\n",
       "        [  9.,  10.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5000,  5.5000])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,\n",
       "         7.5000,  8.5000,  9.5000])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,\n",
       "          0.2689,  0.2689,  0.2689],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-41acb4dd2ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-35-16e7787fa6bf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-16e7787fa6bf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    weights = torch.tensor(np.array([[0.1],[0.2],[0.3],[0.4],[0.5]]),torch.dtype = float)\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor(np.array([[0.1],[0.2],[0.3],[0.4],[0.5]]))\n",
    "st*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

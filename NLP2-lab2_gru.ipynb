{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "# puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 29000\n",
    "\n",
    "#training sets\n",
    "with open('tokenized_low.BPE.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()][:portion]\n",
    "with open('tokenized_low.BPE.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()][:portion]\n",
    "\n",
    "#validation sets\n",
    "with open('val_tokenized_low.BPE.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val_tokenized_low.BPE.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_tokenized.BPE.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_tokenized.BPE.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "#             if w not in puncs:\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m@@\n",
      "m@@\n",
      "563773\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "tokens_list,sentence_list = tokens_sentences(train_en)\n",
    "\n",
    "print(tokens_list[4])\n",
    "print(sentence_list[0][4])\n",
    "\n",
    "print(len(tokens_list))\n",
    "print(len(sorted(set(tokens_list))))\n",
    "# print(set(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 816\n",
      "Vocabulary size FR 862\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "862\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)\n",
    "\n",
    "# print(tokens2id_en['m@@'])\n",
    "# print(id2tokens_en[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the corpora (list of list of ids) simultaneously \n",
    "def convert_corpora2id_both(sentence_list_en, sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length):\n",
    "    \n",
    "    #counts to check long sentences\n",
    "    counter_long = 0\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id_en = []\n",
    "    corpus2id_fr = []\n",
    "    \n",
    "    for s in range(len(sentence_list_en)):\n",
    "    \n",
    "        sentence2id_en = []\n",
    "        sentence2id_en.append(tokens2id_en['<SOS>'])\n",
    "        \n",
    "        sentence2id_fr = []\n",
    "        sentence2id_fr.append(tokens2id_fr['<SOS>'])\n",
    "        \n",
    "        sentence_en = sentence_list_en[s]\n",
    "        sentence_fr = sentence_list_fr[s]\n",
    "        \n",
    "        \n",
    "        for w_en in sentence_en:\n",
    "            word_id = tokens2id_en[w_en]\n",
    "            sentence2id_en.append(word_id)\n",
    "            \n",
    "        for w_fr in sentence_fr:\n",
    "            word_id = tokens2id_fr[w_fr]\n",
    "            sentence2id_fr.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id_en.append(tokens2id_en['<EOS>'])\n",
    "        sentence2id_fr.append(tokens2id_fr['<EOS>'])\n",
    "\n",
    "        if len(sentence2id_en) < max_sentence_length and len(sentence2id_fr) < max_sentence_length:\n",
    "            corpus2id_en.append(sentence2id_en)\n",
    "            corpus2id_fr.append(sentence2id_fr)\n",
    "        \n",
    "        else:\n",
    "            counter_long += 1\n",
    "#             print(sentence_list_en[s])\n",
    "#             print(sentence_list_fr[s])\n",
    "        \n",
    "    print('the number of sentences that were not added is',counter_long)       \n",
    "    return corpus2id_en, corpus2id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "corpus2id_en, corpus2id_fr = convert_corpora2id_both(sentence_list_en,sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  29000 french sentences\n",
      "there are  29000 english sentences\n"
     ]
    }
   ],
   "source": [
    "# print(corpus2id_en[0])\n",
    "\n",
    "print('there are ', len(corpus2id_fr), 'french sentences')\n",
    "print('there are ', len(corpus2id_en), 'english sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get test sentences \n",
    "\n",
    "test_tokens_list_en,test_sentence_list_en = tokens_sentences(test_en)\n",
    "test_tokens_list_fr,test_sentence_list_fr = tokens_sentences(test_fr)\n",
    "\n",
    "test_corpus2id_en, test_corpus2id_fr = convert_corpora2id_both(test_sentence_list_en,test_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr, train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "       \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "       \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.gru = nn.GRU(self.w_embedding_dim,self.w_embedding_dim)\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr, train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        es = ws\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es = (1-self.dropout_prob)*es\n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        output_gru, ht = self.gru(es.unsqueeze(dim=1))\n",
    "        \n",
    "        return stacked_contexts, ht\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        #self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output = (1-self.dropout_prob)*output     \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 3.8911144114239464 0:09:53.132061\n",
      "1 3.1528931910786135 0:09:49.744965\n",
      "2 2.870267925046641 0:09:51.384917\n",
      "3 2.666254452428941 0:10:21.451322\n",
      "4 2.5236281797115145 0:11:39.011356\n",
      "5 2.407500848259905 0:13:37.686077\n",
      "6 2.302968054646562 0:11:24.702286\n",
      "7 2.2085981752869897 0:13:38.264879\n",
      "8 2.1302754523946805 0:14:02.567510\n",
      "9 2.065583769344436 0:12:58.235424\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40483fdb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 29000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_gru_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_gru_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Total loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'un', 'homme', 'dans', 'une', 'chemise', 'bleue', 'se', 'tient', 'sur', 'une', 'échelle', 'pour', 'n@@', 'e@@', 't@@', 't@@', 'o@@', 'y@@', 'er', 'une', 'fenêtre', '.', '<EOS>']\n",
      "24\n",
      "['<SOS>', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window', '.', '<EOS>']\n",
      "['a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a', 'man', 'in', 'a']\n",
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGwAAAEZCAYAAABy/sALAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4048ecc5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair = 3\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_sent(model_NMT,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "french_gold = word_ids2string(test_fr_sentence, id2tokens_fr)\n",
    "print(french_gold)\n",
    "print(len(word_ids2string(test_fr_sentence, id2tokens_fr)))\n",
    "\n",
    "english_gold = word_ids2string(test_en_sentence, id2tokens_en)\n",
    "print(english_gold)\n",
    "\n",
    "english_output = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "print(english_output)\n",
    "print(len(word_ids2string(decoder_outputs, id2tokens_en)))\n",
    "\n",
    "S = decoder_attentions\n",
    "sent_num = pair\n",
    "\n",
    "# visualize_attention(S,sent_num)\n",
    "\n",
    "french_gold = (\" \").join(french_gold)\n",
    "showAttention(french_gold,english_output,S,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sent(model_NMT, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "#         average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "#         decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_NMT(sent_fr, pos_fr, sent_en, train=False)\n",
    "        \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_test_eval(model_NMT, test_sentences_fr, test_sentences_en):\n",
    "\n",
    "    filename = \"test_results.txt\" \n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(2): #range(len(test_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, test_sentences_fr[sent], test_sentences_en[sent])\n",
    "        print(decoder_attentions.size())\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 38])\n",
      "torch.Size([100, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "write_test_eval(model_NMT, test_corpus2id_fr, test_corpus2id_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#BEAM SEARCH\n",
    "#teacher forcing prob\n",
    "#dropout prob\n",
    "#gru lstm rnn check\n",
    "#relu before rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_attention(S,sent_num):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "    #************************************************************************\n",
    "    # S is the log softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "    #************************************************************************\n",
    "\n",
    "    S = S.exp()\n",
    "    \n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy()\n",
    "    numpy_S = numpy_S[:,:,0]\n",
    "    #print(numpy_S.shape)\n",
    "\n",
    "    plt.imshow(numpy_S)\n",
    "    imname = \"attentions-test-\" + str(sent_num)\n",
    "    plt.savefig(imname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions, sentence_number):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    attentions = attentions.exp()\n",
    "    cax = ax.matshow(attentions[:,:,0].data.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    imname = \"attentions-test-\" + str(sentence_number)\n",
    "    plt.savefig(imname)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 3.0693879911807076 0:19:02.360096\n",
      "1 2.3609005282475004 0:18:38.312155\n",
      "2 2.4560820197152164 0:19:25.557234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e2f2262026ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0msent_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#skip SOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0moptimizer_NMT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr,train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "                \n",
    "        self.gru = nn.GRU(self.context_emb_dim,self.context_emb_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr,train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es =  (1-self.dropout_prob)*es\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        output_gru, ht = self.gru(es.unsqueeze(dim=1))\n",
    "        \n",
    "        return stacked_contexts, ht\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                \n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output =  (1-self.dropout_prob)*output         \n",
    "            \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 0.5\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 29000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_grubil_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_grubil_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 4.428850896322727 0:05:15.985931\n",
      "1 3.532055251395702 0:05:26.043657\n",
      "2 3.243277145981789 0:07:02.825930\n",
      "3 3.0481382690131666 0:05:44.152833\n",
      "4 2.8959504411458967 0:05:18.198858\n",
      "5 2.7410687944978473 0:05:05.613343\n",
      "6 2.600624499639869 0:05:11.309431\n",
      "7 2.47786107532084 0:04:51.969705\n",
      "8 2.3710516098409893 0:04:52.959168\n",
      "9 2.279908773472905 0:04:59.686659\n"
     ]
    }
   ],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr,train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr,train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es =  (1-self.dropout_prob)*es\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        return stacked_contexts, average_context\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                \n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output =  (1-self.dropout_prob)*output         \n",
    "            \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 10000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_bil_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_bil_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

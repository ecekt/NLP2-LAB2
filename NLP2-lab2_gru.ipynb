{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "# puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 29000\n",
    "\n",
    "#training sets\n",
    "with open('tokenized_low.BPE.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()][:portion]\n",
    "with open('tokenized_low.BPE.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()][:portion]\n",
    "\n",
    "#validation sets\n",
    "with open('val_tokenized_low.BPE.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val_tokenized_low.BPE.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_tokenized.BPE.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_tokenized.BPE.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "#             if w not in puncs:\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m@@\n",
      "m@@\n",
      "563773\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "tokens_list,sentence_list = tokens_sentences(train_en)\n",
    "\n",
    "print(tokens_list[4])\n",
    "print(sentence_list[0][4])\n",
    "\n",
    "print(len(tokens_list))\n",
    "print(len(sorted(set(tokens_list))))\n",
    "# print(set(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 816\n",
      "Vocabulary size FR 862\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "862\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)\n",
    "\n",
    "# print(tokens2id_en['m@@'])\n",
    "# print(id2tokens_en[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the corpora (list of list of ids) simultaneously \n",
    "def convert_corpora2id_both(sentence_list_en, sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length):\n",
    "    \n",
    "    #counts to check long sentences\n",
    "    counter_long = 0\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id_en = []\n",
    "    corpus2id_fr = []\n",
    "    \n",
    "    for s in range(len(sentence_list_en)):\n",
    "    \n",
    "        sentence2id_en = []\n",
    "        sentence2id_en.append(tokens2id_en['<SOS>'])\n",
    "        \n",
    "        sentence2id_fr = []\n",
    "        sentence2id_fr.append(tokens2id_fr['<SOS>'])\n",
    "        \n",
    "        sentence_en = sentence_list_en[s]\n",
    "        sentence_fr = sentence_list_fr[s]\n",
    "        \n",
    "        \n",
    "        for w_en in sentence_en:\n",
    "            word_id = tokens2id_en[w_en]\n",
    "            sentence2id_en.append(word_id)\n",
    "            \n",
    "        for w_fr in sentence_fr:\n",
    "            word_id = tokens2id_fr[w_fr]\n",
    "            sentence2id_fr.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id_en.append(tokens2id_en['<EOS>'])\n",
    "        sentence2id_fr.append(tokens2id_fr['<EOS>'])\n",
    "\n",
    "        if len(sentence2id_en) < max_sentence_length and len(sentence2id_fr) < max_sentence_length:\n",
    "            corpus2id_en.append(sentence2id_en)\n",
    "            corpus2id_fr.append(sentence2id_fr)\n",
    "        \n",
    "        else:\n",
    "            counter_long += 1\n",
    "#             print(sentence_list_en[s])\n",
    "#             print(sentence_list_fr[s])\n",
    "        \n",
    "    print('the number of sentences that were not added is',counter_long)       \n",
    "    return corpus2id_en, corpus2id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "corpus2id_en, corpus2id_fr = convert_corpora2id_both(sentence_list_en,sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  29000 french sentences\n",
      "there are  29000 english sentences\n"
     ]
    }
   ],
   "source": [
    "# print(corpus2id_en[0])\n",
    "\n",
    "print('there are ', len(corpus2id_fr), 'french sentences')\n",
    "print('there are ', len(corpus2id_en), 'english sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get test sentences \n",
    "\n",
    "test_tokens_list_en,test_sentence_list_en = tokens_sentences(test_en)\n",
    "test_tokens_list_fr,test_sentence_list_fr = tokens_sentences(test_fr)\n",
    "\n",
    "test_corpus2id_en, test_corpus2id_fr = convert_corpora2id_both(test_sentence_list_en,test_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "        self.gru = nn.GRU(self.context_emb_dim,self.context_emb_dim)\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        es = self.dropout(es)\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        output_gru, ht = self.gru(es.unsqueeze(dim=1))\n",
    "        \n",
    "        return stacked_contexts, ht\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        #self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                output = self.dropout(output)\n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 3.0038703128333237 0:17:16.400764\n",
      "1 2.094104791618755 0:21:10.384664\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-74f37a37c73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msent_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#skip SOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.5\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 29000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_gru_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_gru_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Total loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'un', 'homme', 'dans', 'une', 'chemise', 'bleue', 'se', 'tient', 'sur', 'une', 'échelle', 'pour', 'n@@', 'e@@', 't@@', 't@@', 'o@@', 'y@@', 'er', 'une', 'fenêtre', '.', '<EOS>']\n",
      "24\n",
      "['<SOS>', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window', '.', '<EOS>']\n",
      "['a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'in', 'a', 'man', 'in', 'a']\n",
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHwAAAEYCAYAAACeDJJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhobAM4b/D6mkZ73vIhrWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40492c2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair = 3\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_sent(model_NMT,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "french_gold = word_ids2string(test_fr_sentence, id2tokens_fr)\n",
    "print(french_gold)\n",
    "print(len(word_ids2string(test_fr_sentence, id2tokens_fr)))\n",
    "\n",
    "english_gold = word_ids2string(test_en_sentence, id2tokens_en)\n",
    "print(english_gold)\n",
    "\n",
    "english_output = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "print(english_output)\n",
    "print(len(word_ids2string(decoder_outputs, id2tokens_en)))\n",
    "\n",
    "S = decoder_attentions\n",
    "sent_num = pair\n",
    "\n",
    "# visualize_attention(S,sent_num)\n",
    "\n",
    "french_gold = (\" \").join(french_gold)\n",
    "showAttention(french_gold,english_output,S,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sent(model_NMT, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "#         average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "#         decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_NMT(sent_fr, pos_fr, sent_en, train=False)\n",
    "        \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_test_eval(model_NMT, test_sentences_fr, test_sentences_en):\n",
    "\n",
    "    filename = \"test_results.txt\" \n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(2): #range(len(test_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, test_sentences_fr[sent], test_sentences_en[sent])\n",
    "        print(decoder_attentions.size())\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 38])\n",
      "torch.Size([100, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "write_test_eval(model_NMT, test_corpus2id_fr, test_corpus2id_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#BEAM SEARCH\n",
    "#teacher forcing prob\n",
    "#dropout prob\n",
    "#gru lstm rnn check\n",
    "#relu before rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_attention(S,sent_num):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "    #************************************************************************\n",
    "    # S is the log softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "    #************************************************************************\n",
    "\n",
    "    S = S.exp()\n",
    "    \n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy()\n",
    "    numpy_S = numpy_S[:,:,0]\n",
    "    #print(numpy_S.shape)\n",
    "\n",
    "    plt.imshow(numpy_S)\n",
    "    imname = \"attentions-test-\" + str(sent_num)\n",
    "    plt.savefig(imname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions, sentence_number):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    attentions = attentions.exp()\n",
    "    cax = ax.matshow(attentions[:,:,0].data.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    imname = \"attentions-test-\" + str(sentence_number)\n",
    "    plt.savefig(imname)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

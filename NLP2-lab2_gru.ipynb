{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "# puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 29000\n",
    "\n",
    "#training sets\n",
    "with open('tokenized_low.BPE.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()][:portion]\n",
    "with open('tokenized_low.BPE.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()][:portion]\n",
    "\n",
    "#validation sets\n",
    "with open('val_tokenized_low.BPE.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val_tokenized_low.BPE.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_tokenized.BPE.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_tokenized.BPE.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "#             if w not in puncs:\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m@@\n",
      "m@@\n",
      "563773\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "tokens_list,sentence_list = tokens_sentences(train_en)\n",
    "\n",
    "print(tokens_list[4])\n",
    "print(sentence_list[0][4])\n",
    "\n",
    "print(len(tokens_list))\n",
    "print(len(sorted(set(tokens_list))))\n",
    "# print(set(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 816\n",
      "Vocabulary size FR 862\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "862\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)\n",
    "\n",
    "# print(tokens2id_en['m@@'])\n",
    "# print(id2tokens_en[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the corpora (list of list of ids) simultaneously \n",
    "def convert_corpora2id_both(sentence_list_en, sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length):\n",
    "    \n",
    "    #counts to check long sentences\n",
    "    counter_long = 0\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id_en = []\n",
    "    corpus2id_fr = []\n",
    "    \n",
    "    for s in range(len(sentence_list_en)):\n",
    "    \n",
    "        sentence2id_en = []\n",
    "        sentence2id_en.append(tokens2id_en['<SOS>'])\n",
    "        \n",
    "        sentence2id_fr = []\n",
    "        sentence2id_fr.append(tokens2id_fr['<SOS>'])\n",
    "        \n",
    "        sentence_en = sentence_list_en[s]\n",
    "        sentence_fr = sentence_list_fr[s]\n",
    "        \n",
    "        \n",
    "        for w_en in sentence_en:\n",
    "            word_id = tokens2id_en[w_en]\n",
    "            sentence2id_en.append(word_id)\n",
    "            \n",
    "        for w_fr in sentence_fr:\n",
    "            word_id = tokens2id_fr[w_fr]\n",
    "            sentence2id_fr.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id_en.append(tokens2id_en['<EOS>'])\n",
    "        sentence2id_fr.append(tokens2id_fr['<EOS>'])\n",
    "\n",
    "        if len(sentence2id_en) < max_sentence_length and len(sentence2id_fr) < max_sentence_length:\n",
    "            corpus2id_en.append(sentence2id_en)\n",
    "            corpus2id_fr.append(sentence2id_fr)\n",
    "        \n",
    "        else:\n",
    "            counter_long += 1\n",
    "#             print(sentence_list_en[s])\n",
    "#             print(sentence_list_fr[s])\n",
    "        \n",
    "    print('the number of sentences that were not added is',counter_long)       \n",
    "    return corpus2id_en, corpus2id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "corpus2id_en, corpus2id_fr = convert_corpora2id_both(sentence_list_en,sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  29000 french sentences\n",
      "there are  29000 english sentences\n"
     ]
    }
   ],
   "source": [
    "# print(corpus2id_en[0])\n",
    "\n",
    "print('there are ', len(corpus2id_fr), 'french sentences')\n",
    "print('there are ', len(corpus2id_en), 'english sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get test sentences \n",
    "\n",
    "test_tokens_list_en,test_sentence_list_en = tokens_sentences(test_en)\n",
    "test_tokens_list_fr,test_sentence_list_fr = tokens_sentences(test_fr)\n",
    "\n",
    "test_corpus2id_en, test_corpus2id_fr = convert_corpora2id_both(test_sentence_list_en,test_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr, train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "        self.gru = nn.GRU(self.context_emb_dim,self.context_emb_dim)\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr, train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es = self.dropout_prob*es\n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        output_gru, ht = self.gru(es.unsqueeze(dim=1))\n",
    "        \n",
    "        return stacked_contexts, ht\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        #self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output = self.dropout_prob*output     \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    #cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    cj = F.softmax(torch.matmul(sw,torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "\n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 6.650746774673462 0:00:00.358330\n",
      "1 6.49499740600586 0:00:00.301040\n",
      "2 6.265286207199097 0:00:00.336460\n",
      "3 5.768199920654297 0:00:00.349078\n",
      "4 5.327121353149414 0:00:00.451575\n",
      "5 5.084772539138794 0:00:00.377452\n",
      "6 4.952926254272461 0:00:00.449718\n",
      "7 4.860158777236938 0:00:00.427292\n",
      "8 4.786307716369629 0:00:00.417132\n",
      "9 4.724616479873657 0:00:00.313339\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnjCcc87FxROGc865uPw/sgFaRQBh5S8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40483c3588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.5\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 10\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_gru_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_gru_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Total loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'un', 'homme', 'dans', 'une', 'chemise', 'bleue', 'se', 'tient', 'sur', 'une', 'échelle', 'pour', 'n@@', 'e@@', 't@@', 't@@', 'o@@', 'y@@', 'er', 'une', 'fenêtre', '.', '<EOS>']\n",
      "24\n",
      "['<SOS>', 'a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window', '.', '<EOS>']\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHYAAAEYCAYAAACJLgKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2IZiFdtQrGIbyv8DkGsMNHQnMl4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40375d2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair = 3\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_sent(model_NMT,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "french_gold = word_ids2string(test_fr_sentence, id2tokens_fr)\n",
    "print(french_gold)\n",
    "print(len(word_ids2string(test_fr_sentence, id2tokens_fr)))\n",
    "\n",
    "english_gold = word_ids2string(test_en_sentence, id2tokens_en)\n",
    "print(english_gold)\n",
    "\n",
    "english_output = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "print(english_output)\n",
    "print(len(word_ids2string(decoder_outputs, id2tokens_en)))\n",
    "\n",
    "S = decoder_attentions\n",
    "sent_num = pair\n",
    "\n",
    "# visualize_attention(S,sent_num)\n",
    "\n",
    "french_gold = (\" \").join(french_gold)\n",
    "showAttention(french_gold,english_output,S,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sent(model_NMT, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "#         average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "#         decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_NMT(sent_fr, pos_fr, sent_en, train=False)\n",
    "        \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_test_eval(model_NMT, test_sentences_fr, test_sentences_en):\n",
    "\n",
    "    filename = \"test_results.txt\" \n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(2): #range(len(test_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, test_sentences_fr[sent], test_sentences_en[sent])\n",
    "        print(decoder_attentions.size())\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 38])\n",
      "torch.Size([100, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "write_test_eval(model_NMT, test_corpus2id_fr, test_corpus2id_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#BEAM SEARCH\n",
    "#teacher forcing prob\n",
    "#dropout prob\n",
    "#gru lstm rnn check\n",
    "#relu before rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_attention(S,sent_num):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "    #************************************************************************\n",
    "    # S is the log softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "    #************************************************************************\n",
    "\n",
    "    S = S.exp()\n",
    "    \n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy()\n",
    "    numpy_S = numpy_S[:,:,0]\n",
    "    #print(numpy_S.shape)\n",
    "\n",
    "    plt.imshow(numpy_S)\n",
    "    imname = \"attentions-test-\" + str(sent_num)\n",
    "    plt.savefig(imname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions, sentence_number):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    attentions = attentions.exp()\n",
    "    cax = ax.matshow(attentions[:,:,0].data.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    imname = \"attentions-test-\" + str(sentence_number)\n",
    "    plt.savefig(imname)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

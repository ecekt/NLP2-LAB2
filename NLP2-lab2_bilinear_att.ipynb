{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "# puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 29000\n",
    "\n",
    "#training sets\n",
    "with open('tokenized_low.BPE.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()][:portion]\n",
    "with open('tokenized_low.BPE.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()][:portion]\n",
    "\n",
    "#validation sets\n",
    "with open('val_tokenized_low.BPE.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val_tokenized_low.BPE.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_tokenized.BPE.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_tokenized.BPE.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "#             if w not in puncs:\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m@@\n",
      "m@@\n",
      "563773\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "tokens_list,sentence_list = tokens_sentences(train_en)\n",
    "\n",
    "print(tokens_list[4])\n",
    "print(sentence_list[0][4])\n",
    "\n",
    "print(len(tokens_list))\n",
    "print(len(sorted(set(tokens_list))))\n",
    "# print(set(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 816\n",
      "Vocabulary size FR 862\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "862\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)\n",
    "\n",
    "# print(tokens2id_en['m@@'])\n",
    "# print(id2tokens_en[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the corpora (list of list of ids) simultaneously \n",
    "def convert_corpora2id_both(sentence_list_en, sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length):\n",
    "    \n",
    "    #counts to check long sentences\n",
    "    counter_long = 0\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id_en = []\n",
    "    corpus2id_fr = []\n",
    "    \n",
    "    for s in range(len(sentence_list_en)):\n",
    "    \n",
    "        sentence2id_en = []\n",
    "        sentence2id_en.append(tokens2id_en['<SOS>'])\n",
    "        \n",
    "        sentence2id_fr = []\n",
    "        sentence2id_fr.append(tokens2id_fr['<SOS>'])\n",
    "        \n",
    "        sentence_en = sentence_list_en[s]\n",
    "        sentence_fr = sentence_list_fr[s]\n",
    "        \n",
    "        \n",
    "        for w_en in sentence_en:\n",
    "            word_id = tokens2id_en[w_en]\n",
    "            sentence2id_en.append(word_id)\n",
    "            \n",
    "        for w_fr in sentence_fr:\n",
    "            word_id = tokens2id_fr[w_fr]\n",
    "            sentence2id_fr.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id_en.append(tokens2id_en['<EOS>'])\n",
    "        sentence2id_fr.append(tokens2id_fr['<EOS>'])\n",
    "\n",
    "        if len(sentence2id_en) < max_sentence_length and len(sentence2id_fr) < max_sentence_length:\n",
    "            corpus2id_en.append(sentence2id_en)\n",
    "            corpus2id_fr.append(sentence2id_fr)\n",
    "        \n",
    "        else:\n",
    "            counter_long += 1\n",
    "#             print(sentence_list_en[s])\n",
    "#             print(sentence_list_fr[s])\n",
    "        \n",
    "    print('the number of sentences that were not added is',counter_long)       \n",
    "    return corpus2id_en, corpus2id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "corpus2id_en, corpus2id_fr = convert_corpora2id_both(sentence_list_en,sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  29000 french sentences\n",
      "there are  29000 english sentences\n"
     ]
    }
   ],
   "source": [
    "# print(corpus2id_en[0])\n",
    "\n",
    "print('there are ', len(corpus2id_fr), 'french sentences')\n",
    "print('there are ', len(corpus2id_en), 'english sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get test sentences \n",
    "\n",
    "test_tokens_list_en,test_sentence_list_en = tokens_sentences(test_en)\n",
    "test_tokens_list_fr,test_sentence_list_fr = tokens_sentences(test_fr)\n",
    "\n",
    "test_corpus2id_en, test_corpus2id_fr = convert_corpora2id_both(test_sentence_list_en,test_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr,train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr,train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es = self.dropout_prob*es\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        return stacked_contexts, average_context\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                \n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output = self.dropout_prob*output         \n",
    "            \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 6.662420749664307 0:00:00.293661\n",
      "1 6.509496212005615 0:00:00.256591\n",
      "2 6.291938304901123 0:00:00.254488\n",
      "3 5.816521310806275 0:00:00.256576\n",
      "4 5.367209005355835 0:00:00.244850\n",
      "5 5.112206935882568 0:00:00.325073\n",
      "6 4.9715667247772215 0:00:00.248437\n",
      "7 4.878013229370117 0:00:00.311699\n",
      "8 4.805761814117432 0:00:00.304984\n",
      "9 4.746307516098023 0:00:00.249389\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe847763fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.5\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 10\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_bil_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_bil_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Total loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"bil_mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair = 3\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_sent(model_NMT,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "french_gold = word_ids2string(test_fr_sentence, id2tokens_fr)\n",
    "print(french_gold)\n",
    "print(len(word_ids2string(test_fr_sentence, id2tokens_fr)))\n",
    "\n",
    "english_gold = word_ids2string(test_en_sentence, id2tokens_en)\n",
    "print(english_gold)\n",
    "\n",
    "english_output = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "print(english_output)\n",
    "print(len(word_ids2string(decoder_outputs, id2tokens_en)))\n",
    "\n",
    "S = decoder_attentions\n",
    "sent_num = pair\n",
    "\n",
    "# visualize_attention(S,sent_num)\n",
    "\n",
    "french_gold = (\" \").join(french_gold)\n",
    "showAttention(french_gold,english_output,S,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sent(model_NMT, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "#         average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "#         decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_NMT(sent_fr, pos_fr, sent_en, train=False)\n",
    "        \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_test_eval(model_NMT, test_sentences_fr, test_sentences_en):\n",
    "\n",
    "    filename = \"test_results.txt\" \n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(2): #range(len(test_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, test_sentences_fr[sent], test_sentences_en[sent])\n",
    "        print(decoder_attentions.size())\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 38])\n",
      "torch.Size([100, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "write_test_eval(model_NMT, test_corpus2id_fr, test_corpus2id_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#BEAM SEARCH\n",
    "#teacher forcing prob\n",
    "#dropout prob\n",
    "#gru lstm rnn check\n",
    "#relu before rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_attention(S,sent_num):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "    #************************************************************************\n",
    "    # S is the log softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "    #************************************************************************\n",
    "\n",
    "    S = S.exp()\n",
    "    \n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy()\n",
    "    numpy_S = numpy_S[:,:,0]\n",
    "    #print(numpy_S.shape)\n",
    "\n",
    "    plt.imshow(numpy_S)\n",
    "    imname = \"attentions-test-\" + str(sent_num)\n",
    "    plt.savefig(imname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions, sentence_number):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    attentions = attentions.exp()\n",
    "    cax = ax.matshow(attentions[:,:,0].data.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    imname = \"attentions-test-\" + str(sentence_number)\n",
    "    plt.savefig(imname)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training sets\n",
    "with open('train.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()]\n",
    "with open('train.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#validation sets\n",
    "with open('val.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_2017_flickr.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_2017_flickr.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 15460\n",
      "Vocabulary size FR 17007\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15460\n",
      "17007\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_corpus2id(sentence_list, tokens2id):\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id = []\n",
    "    \n",
    "    for s in sentence_list:\n",
    "    \n",
    "        sentence2id = []\n",
    "        sentence2id.append(tokens2id['<SOS>'])\n",
    "    \n",
    "        for w in s:\n",
    "            word_id = tokens2id[w]\n",
    "            sentence2id.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id.append(tokens2id['<EOS>'])\n",
    "        corpus2id.append(sentence2id)\n",
    "    \n",
    "    return corpus2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus2id_en = convert_corpus2id(sentence_list_en, tokens2id_en)\n",
    "corpus2id_fr = convert_corpus2id(sentence_list_fr, tokens2id_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2021, 15433, 2129, 8939, 2555, 9954, 9572, 8983, 3797, 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2id_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 -4.364972195637165 0:00:02.209448\n",
      "1 -13.319249298423529 0:00:02.369683\n",
      "2 -13.468881700187922 0:00:02.195289\n",
      "3 -13.480550304055214 0:00:02.325352\n",
      "4 -13.48816978186369 0:00:02.192654\n",
      "5 -13.490378014743328 0:00:02.185991\n",
      "6 -13.492605973035097 0:00:02.040180\n",
      "7 -13.494429521262646 0:00:02.284919\n",
      "8 -13.495641734451056 0:00:01.989770\n",
      "9 -13.496985681355 0:00:02.134258\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMxq+v+qHFVrHmJAxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16997940f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "        \n",
    "        self.average_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        \n",
    "        self.attention_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        #do we use non-linearity after attention\n",
    "        \n",
    "        #TODO: DROPOUT\n",
    "        \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "        \n",
    "        ws = []\n",
    "        ps = []\n",
    "        es = []\n",
    "        \n",
    "        for s in range(len(sent_fr)):\n",
    "            word = sent_fr[s]\n",
    "            pos = pos_fr[s]\n",
    "            \n",
    "            w_out = self.w_embeddings(word)\n",
    "\n",
    "            p_out = self.p_embeddings(pos)\n",
    "\n",
    "            e_out = torch.cat((w_out, p_out), 0)\n",
    "    \n",
    "            ws.append(w_out)\n",
    "            ps.append(p_out)\n",
    "            es.append(e_out)\n",
    "        \n",
    "        stacked_contexts = torch.stack(es, dim = 0)\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        average_context = self.average_projection(average_context)\n",
    "        \n",
    "        stacked_contexts = self.attention_projection(stacked_contexts)\n",
    "            \n",
    "        return average_context, stacked_contexts\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "        #self.bidirLSTM = nn.LSTM(self.embedding_dim, self.embedding_dim, bidirectional=True)\n",
    "        #TODO: LSTM, GRU \n",
    "       \n",
    "        self.pre_rnn_affine = nn.Linear(self.dec_embedding_dim*2, self.dec_embedding_dim)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_avg_context, encoder_stacked_contexts):\n",
    "        \n",
    "        pred = []\n",
    "        attentions = []\n",
    "        \n",
    "        for s in range(len(gold_target_sent)):\n",
    "            gold_word = gold_target_sent[s]\n",
    "            \n",
    "            output = self.embedding(gold_word)\n",
    "\n",
    "            #g_out = self.dropout(g_out)\n",
    "            \n",
    "            if s == 0:\n",
    "                \n",
    "                weighted_context = torch.zeros(output.shape)\n",
    "                output = torch.cat((output, weighted_context), 0)\n",
    "                \n",
    "                output = self.pre_rnn_affine(output)\n",
    "                #TODO: start with 0 vector as h0\n",
    "                \n",
    "                output, hidden = self.rnn(output.view(1, 1, -1), encoder_avg_context.view(1, 1, -1))\n",
    "                prev_hidden = hidden\n",
    "                \n",
    "                s_output = self.out_affine(output[0])\n",
    "                s_output = F.softmax(s_output, dim=1) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "                \n",
    "                pred.append(s_output)\n",
    "                \n",
    "            elif s == len(gold_target_sent)-1:\n",
    "                \n",
    "                #end of sentence\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                            \n",
    "                #TODO: how to add weighted context\n",
    "                output = torch.cat((output, weighted_context), 0)\n",
    "                \n",
    "                output = self.pre_rnn_affine(output)\n",
    "                \n",
    "                output, hidden = self.rnn(output.view(1, 1, -1), prev_hidden.view(1, 1, -1))\n",
    "                prev_hidden = hidden\n",
    "                \n",
    "                s_output = self.out_affine(output[0])\n",
    "                \n",
    "                s_output = F.softmax(s_output, dim=1) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "                pred.append(s_output)\n",
    "                            \n",
    "            attention_weights_word = F.softmax(torch.matmul(encoder_stacked_contexts, prev_hidden.view(-1,1)), dim = 0)\n",
    "           \n",
    "            #print(attention_weights_word)\n",
    "           \n",
    "            weighted_context = torch.sum(torch.mul(attention_weights_word, encoder_stacked_contexts), dim = 0)\n",
    "            \n",
    "            attentions.append(attention_weights_word)\n",
    "            \n",
    "            \n",
    "        attention_weights = torch.stack(attentions, dim=0)\n",
    "            \n",
    "        pred = torch.stack(pred, dim=0)\n",
    "        \n",
    "        return pred, attention_weights #output, hidden \n",
    "\n",
    "    \n",
    "    #https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "w_embedding_dim = 10\n",
    "p_embedding_dim = 10\n",
    "dec_embedding_dim = 10\n",
    "dropout_prob = 0.1\n",
    "\n",
    "model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "\n",
    "optimizer_encoder = optim.Adam(model_encoder.parameters(), lr = learning_rate)\n",
    "optimizer_decoder = optim.Adam(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 100\n",
    "\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    " \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "           \n",
    "            sent_fr_pos = torch.tensor(np.asarray(pos_fr), dtype= torch.long)            \n",
    "            sent_en_pos = torch.tensor(np.asarray(pos_en), dtype= torch.long)\n",
    "            \n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "\n",
    "            average_context, stacked_contexts = model_encoder(sent_fr, sent_fr_pos)\n",
    "        \n",
    "            pred, attention_weights = model_decoder(sent_en, average_context, stacked_contexts)\n",
    "            \n",
    "            #print(pred, sent_en)\n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred.squeeze(), sent_en)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_encoder.step()\n",
    "            optimizer_decoder.step()\n",
    "\n",
    "            total_loss += loss.item() \n",
    "        \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    print(e, total_loss, now-then)\n",
    "    \n",
    "\n",
    "with open('model_encoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_encoder,file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_decoder,file)\n",
    "    \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 100\n",
    "\n",
    "with open('model_encoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_encoder = pickle.load(file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_attention(model, sentence):\n",
    "    \n",
    "#************************************************************************\n",
    "# A is the attention torch Tensor: the output of your model\n",
    "# S is the softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "#************************************************************************\n",
    "\n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy() # get the data in Variable, and then the torch Tensor as numpy array\n",
    "    plt.imshow(numpy_S)\n",
    "    plt.savefig(\"attention-sent-{}-epoch-{}\".format(i, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model_encoder, sent_fr, sent_en, max_sent_length):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "    \n",
    "    return translation, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#TEST - INFERENCE\n",
    "#BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(np.asarray([i for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,\n",
       "         0.7000,  0.8000,  0.9000])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.tensor(np.asarray([i+1 for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "        [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "        [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.],\n",
      "        [  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.]])\n"
     ]
    }
   ],
   "source": [
    "st = torch.stack([a,b,a,b,a], dim = 0)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,\n",
       "          0.7000,  0.8000,  0.9000],\n",
       "        [ 0.2000,  0.4000,  0.6000,  0.8000,  1.0000,  1.2000,  1.4000,\n",
       "          1.6000,  1.8000,  2.0000]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor(np.asarray([0.1, 0.2]), dtype = torch.float).view(-1,1)\n",
    "print(weights.shape)\n",
    "\n",
    "torch.mul(weights, st)\n",
    "# torch.matmul(weights.view(1,2), st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.],\n",
       "        [  1.,   2.],\n",
       "        [  2.,   3.],\n",
       "        [  3.,   4.],\n",
       "        [  4.,   5.],\n",
       "        [  5.,   6.],\n",
       "        [  6.,   7.],\n",
       "        [  7.,   8.],\n",
       "        [  8.,   9.],\n",
       "        [  9.,  10.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5000,  5.5000])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,\n",
       "         7.5000,  8.5000,  9.5000])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,\n",
       "          0.2689,  0.2689,  0.2689],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:len(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-35-16e7787fa6bf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-16e7787fa6bf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    weights = torch.tensor(np.array([[0.1],[0.2],[0.3],[0.4],[0.5]]),torch.dtype = float)\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor(np.array([[0.1],[0.2],[0.3],[0.4],[0.5]]))\n",
    "st*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

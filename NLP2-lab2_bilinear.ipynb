{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "# puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portion = 29000\n",
    "\n",
    "#training sets\n",
    "with open('tokenized_low.BPE.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()][:portion]\n",
    "with open('tokenized_low.BPE.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()][:portion]\n",
    "\n",
    "#validation sets\n",
    "with open('val_tokenized_low.BPE.en') as f:\n",
    "    val_en = [l.strip() for l in f.readlines()]\n",
    "with open('val_tokenized_low.BPE.fr') as f:\n",
    "    val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "#test sets\n",
    "with open('test_tokenized.BPE.en') as f:\n",
    "    test_en = [l.strip() for l in f.readlines()]\n",
    "with open('test_tokenized.BPE.fr') as f:\n",
    "    test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "#             if w not in puncs:\n",
    "            tokens_list.append(w)\n",
    "            sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m@@\n",
      "m@@\n",
      "563773\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "tokens_list,sentence_list = tokens_sentences(train_en)\n",
    "\n",
    "print(tokens_list[4])\n",
    "print(sentence_list[0][4])\n",
    "\n",
    "print(len(tokens_list))\n",
    "print(len(sorted(set(tokens_list))))\n",
    "# print(set(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 816\n",
      "Vocabulary size FR 862\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "862\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)\n",
    "\n",
    "# print(tokens2id_en['m@@'])\n",
    "# print(id2tokens_en[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#building the corpora (list of list of ids) simultaneously \n",
    "def convert_corpora2id_both(sentence_list_en, sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length):\n",
    "    \n",
    "    #counts to check long sentences\n",
    "    counter_long = 0\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id_en = []\n",
    "    corpus2id_fr = []\n",
    "    \n",
    "    for s in range(len(sentence_list_en)):\n",
    "    \n",
    "        sentence2id_en = []\n",
    "        sentence2id_en.append(tokens2id_en['<SOS>'])\n",
    "        \n",
    "        sentence2id_fr = []\n",
    "        sentence2id_fr.append(tokens2id_fr['<SOS>'])\n",
    "        \n",
    "        sentence_en = sentence_list_en[s]\n",
    "        sentence_fr = sentence_list_fr[s]\n",
    "        \n",
    "        \n",
    "        for w_en in sentence_en:\n",
    "            word_id = tokens2id_en[w_en]\n",
    "            sentence2id_en.append(word_id)\n",
    "            \n",
    "        for w_fr in sentence_fr:\n",
    "            word_id = tokens2id_fr[w_fr]\n",
    "            sentence2id_fr.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id_en.append(tokens2id_en['<EOS>'])\n",
    "        sentence2id_fr.append(tokens2id_fr['<EOS>'])\n",
    "\n",
    "        if len(sentence2id_en) < max_sentence_length and len(sentence2id_fr) < max_sentence_length:\n",
    "            corpus2id_en.append(sentence2id_en)\n",
    "            corpus2id_fr.append(sentence2id_fr)\n",
    "        \n",
    "        else:\n",
    "            counter_long += 1\n",
    "#             print(sentence_list_en[s])\n",
    "#             print(sentence_list_fr[s])\n",
    "        \n",
    "    print('the number of sentences that were not added is',counter_long)       \n",
    "    return corpus2id_en, corpus2id_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "corpus2id_en, corpus2id_fr = convert_corpora2id_both(sentence_list_en,sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  29000 french sentences\n",
      "there are  29000 english sentences\n"
     ]
    }
   ],
   "source": [
    "# print(corpus2id_en[0])\n",
    "\n",
    "print('there are ', len(corpus2id_fr), 'french sentences')\n",
    "print('there are ', len(corpus2id_en), 'english sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get test sentences \n",
    "\n",
    "test_tokens_list_en,test_sentence_list_en = tokens_sentences(test_en)\n",
    "test_tokens_list_fr,test_sentence_list_fr = tokens_sentences(test_fr)\n",
    "\n",
    "test_corpus2id_en, test_corpus2id_fr = convert_corpora2id_both(test_sentence_list_en,test_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_sent(model_NMT, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "#         average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "#         decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_NMT(sent_fr, pos_fr, sent_en, train=False)\n",
    "        \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_test_eval(model_NMT, test_sentences_fr, test_sentences_en, portion, e, label):\n",
    "\n",
    "    filename = 'test_results_' + portion + '_' + e + '_' + label + '.txt'\n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(len(test_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, test_sentences_fr[sent], test_sentences_en[sent])\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_attention(S,sent_num):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "    #************************************************************************\n",
    "    # S is the log softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "    #************************************************************************\n",
    "\n",
    "    S = S.exp()\n",
    "    \n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy()\n",
    "    numpy_S = numpy_S[:,:,0]\n",
    "    #print(numpy_S.shape)\n",
    "\n",
    "    plt.imshow(numpy_S)\n",
    "    imname = \"attentions-test-\" + str(sent_num)\n",
    "    plt.savefig(imname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions, sentence_number):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    attentions = attentions.exp()\n",
    "    cax = ax.matshow(attentions[:,:,0].data.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    imname = \"attentions-test-\" + str(sentence_number)\n",
    "    plt.savefig(imname)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-10e7c10f4da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0msent_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#skip SOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0moptimizer_NMT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#bilinear\n",
    "\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr,train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "                \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr,train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es =  (1-self.dropout_prob)*es\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        return stacked_contexts, average_context\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                \n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output =  (1-self.dropout_prob)*output         \n",
    "            \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 10000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_bil_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_bil_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bilinear with gru\n",
    "\n",
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length,dropout_prob)\n",
    "        self.decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "            \n",
    "    def forward(self, sent_fr, pos_fr, sent_en, train):\n",
    "        \n",
    "        stacked_contexts, ht = self.encoder(sent_fr, pos_fr,train)\n",
    "        \n",
    "        pred, attention_weights = self.decoder(sent_en, stacked_contexts, ht, train)\n",
    "          \n",
    "        return pred, attention_weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, dropout_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        initrange = 0.5 / self.w_embedding_dim\n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.p_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "                \n",
    "        self.gru = nn.GRU(self.context_emb_dim,self.context_emb_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr,train):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "       \n",
    "        ws = self.w_embeddings(sent_fr)\n",
    "        ps = self.p_embeddings(pos_fr)\n",
    "        es = torch.cat((ws, ps), 1)\n",
    "        \n",
    "        if train:\n",
    "            es = self.dropout(es)\n",
    "        else:\n",
    "            es =  (1-self.dropout_prob)*es\n",
    "        \n",
    "        stacked_contexts = es\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        \n",
    "        output_gru, ht = self.gru(es.unsqueeze(dim=1))\n",
    "        \n",
    "        return stacked_contexts, ht\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim*2\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        initrange = 0.5 / self.dec_embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)        \n",
    "        \n",
    "        self.lstm = nn.LSTM(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "       \n",
    "        self.bilinear_att = nn.Linear(self.dec_embedding_dim, self.dec_embedding_dim, bias = False)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim*2, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_stacked_contexts, encoder_avg_context, train):\n",
    "        \n",
    "        if train:\n",
    "            pred = []\n",
    "            attentions = []\n",
    "\n",
    "            embeds = self.embedding(gold_target_sent)\n",
    "            embeds = self.dropout(embeds)\n",
    "\n",
    "            output, (hidden, cell) = self.lstm(embeds.view(-1,1,self.dec_embedding_dim ),(encoder_avg_context.view(1, 1, -1), encoder_avg_context.view(1,1,-1)))\n",
    "\n",
    "            #print(output[-1], hidden) same\n",
    "            \n",
    "            \n",
    "            for w in range(len(gold_target_sent)-1):\n",
    "\n",
    "                sw = output[w]\n",
    "                \n",
    "                cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                \n",
    "                c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "                \n",
    "                c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "                \n",
    "                sw = torch.cat((sw, c_vec),dim=1)\n",
    "                \n",
    "                s_output = self.out_affine(sw)\n",
    "                s_output = F.log_softmax(s_output, dim=1)\n",
    "\n",
    "                pred.append(s_output)\n",
    "                \n",
    "                attentions.append(cj)\n",
    "                \n",
    "            attentions = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=1)\n",
    "\n",
    "            return pred, attentions\n",
    "        \n",
    "        \n",
    "        else: #test\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "                \n",
    "                output =  (1-self.dropout_prob)*output         \n",
    "            \n",
    "            \n",
    "                if w == 0:\n",
    "            \n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (encoder_avg_context.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "                \n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                else:\n",
    "                    output, (hidden,cell) = self.lstm(output.view(1, 1, -1), (prev_hidden.view(1, 1, -1),encoder_avg_context.view(1, 1, -1)))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    sw = output[0]\n",
    "                    \n",
    "                    cj = F.softmax(torch.matmul(self.bilinear_att(sw),torch.transpose(encoder_stacked_contexts,0,1)), dim = 1)\n",
    "                    \n",
    "                    c_vec = cj.view(-1,1) *  encoder_stacked_contexts.squeeze()\n",
    "\n",
    "                    c_vec = torch.sum(c_vec, dim=0).view(1,-1)\n",
    "\n",
    "                    sw = torch.cat((sw, c_vec),dim=1)\n",
    "\n",
    "                    s_output = self.out_affine(sw)\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                 \n",
    "                decoder_attentions.append(cj)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "            decoder_attentions = torch.stack(decoder_attentions, dim=0)\n",
    "                \n",
    "            return decoder_outputs, decoder_attentions\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.5\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "# model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "# model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "model_NMT = NMTModel(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length, vocab_size_en, dropout_prob)\n",
    "\n",
    "optimizer_NMT = optim.SGD(model_NMT.parameters(), lr = learning_rate)\n",
    "# optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "# optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 29000\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_NMT.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "        \n",
    "            pred, attention_weights = model_NMT(sent_fr, pos_fr, sent_en, train)\n",
    "            \n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred[0], sent_en)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_NMT.step()\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "       \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss/portion)\n",
    "    \n",
    "    print(e, total_loss/portion, now-then)\n",
    "\n",
    "    with open('model_NMT_grubil_' + str(portion) + '_'+str(e)+'.pickle','wb') as file:\n",
    "        pickle.dump(model_NMT,file)\n",
    "        \n",
    "    \n",
    "with open('loss_grubil_' + str(portion) + '_' +str(e) + '.txt','wb') as file:\n",
    "    pickle.dump(losses,file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair = 3\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_sent(model_NMT,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "french_gold = word_ids2string(test_fr_sentence, id2tokens_fr)\n",
    "print(french_gold)\n",
    "print(len(word_ids2string(test_fr_sentence, id2tokens_fr)))\n",
    "\n",
    "english_gold = word_ids2string(test_en_sentence, id2tokens_en)\n",
    "print(english_gold)\n",
    "\n",
    "english_output = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "print(english_output)\n",
    "print(len(word_ids2string(decoder_outputs, id2tokens_en)))\n",
    "\n",
    "S = decoder_attentions\n",
    "sent_num = pair\n",
    "\n",
    "# visualize_attention(S,sent_num)\n",
    "\n",
    "french_gold = (\" \").join(french_gold)\n",
    "showAttention(french_gold,english_output,S,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl8VNX5+PHPk4WEJQskYUkChB2U\nJawCIiKuuKHVilvVLlqsy9e2tl9t+2ut3fSrVVtttda2am1dihsuuFBWRZAdwh4gQAhLFkIIELI9\nvz/OTRhiNpJJJpM879drXpm5c+fcJ3fuzDPnnHvPEVXFGGOMaYiQQAdgjDEmeFkSMcYY02CWRIwx\nxjSYJRFjjDENZknEGGNMg1kSMcYY02CWREzQEJE7ReSAiBSKSFwzbvcnIvJCc23PZ7tXi8ge7/8d\nWY/1p4hIpp+2fZOIfOKPshoRw3Mi8v8CGYOpm9h1IsFLRDKA76jq3EDH0tREJBwoAMar6tom3M4U\n4BVVTW6qbZxGLNuBH6jquzU8r8AAVU33Hk+hiWKvuq0mKP823LE8qSnKN03HaiImWHQDIoENgQ6k\nGfWmFfy/IhIW6BhM07Ek0kqJyO0iki4ieSIyW0QSveUiIk+KyEEROSwi60RkqPfcpSKyUUSOiMhe\nEbm/hrL7icg8EckVkRwR+ZeIxPo8/7/e64+IyBYROb+Gci4TkdUiUuA12zxUw3oDgS3ew3xv2yki\nor5fUCKyQES+492/TUQ+E5HHReSQiOwUkWk+63YRkX+ISJb3/Dsi0hGYAyR6TUiFIpIoIg+JyCs+\nr71SRDaISL63zSE+z2WIyP3efj0sIq+LSGQN/1eIiPxMRHZ578fLIhIjIhEiUgiEAmu9GknV1y7y\n7q714pzh89wPvfL2icg3fZZHePtjt9cs+JyItK8htttE5LPatiUil4vIGm8/LBGR4VX2w/+KyDrg\nqIiEicgDIrLdOy42isjV3rpDgOeACV75+d7yF0Xk1z5lVntMe8+piMwUkW3e+/knEZHq/jfjZ6pq\ntyC9ARnABdUsnwrkAKOACOBpYJH33MXASiAWEGAI0MN7bh9wjne/MzCqhu32By70yk4AFgFPec8N\nAvYAid7jFKBfDeVMAYbhfswMBw4AV9WwbgqgQFh1j71lC3BNIgC3ASXA7bgv4zuBLE424X4AvO79\nn+HAuT4xZVbZ9kO4ZiKAgcBR7/8PB34MpAPtfN6TL4FEoAuwCZhZw//0Le+1fYFOwFvAP32eV6B/\nLe//Kc97sZcCD3uxXQocAzp7zz8FzPbiigLeA35XQ9m3AZ/Vsq1RwEHgLG//3ur97xE++2EN0BNo\n7y37urdfQoAZ3n7sUd32vGUvAr+u65j2ie993HHdC8gGLgn0Z7Qt3Kwm0jrdBPxdVVep6gngQdyv\nvBTcF2sUMBj3hbpJVfd5rysBzhCRaFU9pKqrqitcVdNV9VNVPaGq2cATwLne02W4D/kZIhKuqhmq\n+pVf0l45C1R1vaqWq+o64FWfcvxhl6r+VVXLgJeAHkA3EekBTMN9uR9S1RJVXVjPMmcAH3j/fwnw\nONAemOizzh9VNUtV83Bf1Kk1lHUT8ISq7lDVQtz7dL00rvmnBHjY+58+BAqBQd6v8tuB76tqnqoe\nAX4LXN/A7dwO/EVVl6lqmaq+BJwAxvus80dV3aOqxwFU9T/efilX1deBbcC4em6vtmO6wiOqmq+q\nu4H51LzfjR9ZEmmdEoFdFQ+8L6hcIElV5wHPAH8CDojI8yIS7a16De7X6y4RWSgiE6orXES6ishr\nXpNVAfAKEO9tKx24D/fr/aC3XmIN5ZwlIvNFJFtEDgMzK8rxk/0Vd1T1mHe3E+7XcZ6qHmpAmVX3\nbTmu5pVU3XZxNYFO9SnLux+G6/9pqFxVLa1m+wlAB2Cl1/yUD3zkLW+I3sAPK8ryyuuJ+58q7PF9\ngYjc4tP8lQ8Mpf7vd43HtM869d3vxo8sibROWbgPOQBeW38csBdAVf+oqqOBM3HNMz/yli9X1elA\nV+Ad4I0ayv8drvlguKpGAzfjmsbwyvm3urNsenvrPVpDOf/GNa/0VNUYXLt4fduxj3p/O/gs617P\n1+4BuohPP46Puk5XrLpvBfflubee266xLFwzTCmuWc/fcoDjwJmqGuvdYlS1oV+0e4Df+JQVq6od\nVPVVn3Uq96WI9Ab+CtwNxKlqLJDGyff7dPf7Kce0CRxLIsEvXEQifW5huC/nb4pIqohE4Jotlqlq\nhoiM9WoA4bgv4iKgTETaibs2IMZrpinANU1VJwrXTJIvIkl4SQhARAaJyFRvu0W4L67ayslT1SIR\nGQfcWN9/2mtG2wvcLCKhIvItoF89X7sP14H+ZxHpLCLhIjLZe/oAECciMTW8/A3gMhE539uHP8Q1\n4yypb+w+XgW+LyJ9RKQT7n16vUpNojYHcP0pdfJqTH8FnhSRrgAikiQiFzdwW38FZnrHkohIR3En\nSkTV8PqOuESR7W37m7iaiG/5ySLSrobX13hM1zN+00QsiQS/D3Ff1BW3h1T1v8D/A97EdZb342Tb\ndzTuC+AQrnkgF9euD/ANIMNropqJq2FU55e4Ds7DuA7qt3yeiwAewf3y3Y+r1fykhnK+BzwsIkeA\nn1Nzzacmt+MSWC6uVnU6X+TfwPUfbMZ1EN8HoKqbcV/uO7xml1Oa4lR1C26/PI37H68ArlDV4tOM\nHeDvwD9xJybsxCXde07j9Q8BL3lxXleP9f8X15G/1HuP5+JOhDjtbanqCtz+fwZ3LKXjOserpaob\ngd8DX+ASxjDgc59V5uFOZ94vIjnVvL62Y9oEkF1saIwxpsGsJmKMMabBLIkYY4xpMEsixhhjGsyS\niDHGmAYLuoHR4uPjNSUlJdBhGGNMUFm5cmWOqjb04tIaNXkSEZFQYAWwV1Uvr/LcbcBjnLxg6BlV\nrXXehpSUFFasWNEUoRpjTKslIrvqXuv0NUdN5H9wg9BF1/D866p6dzPEYYwxxs+atE9ERJKBy4Bm\nnxXOGGNM02vqjvWncENll9eyzjXi5l6YJSI9q1tBRO4QkRUisiI7O7tJAjXGGHP6mqw5S0QuBw6q\n6kpx03ZW5z3gVVU9ISIzccN1T626kqo+DzwPMGbMGLvE3hgDQElJCZmZmRQVFQU6lBYjMjKS5ORk\nwsPDm2V7TdkncjZwpYhcipvWNFpEXlHVyvGYVDXXZ/2/UvNor8YY8xWZmZlERUWRkpKCTWToJhnM\nzc0lMzOTPn36NMs2m6w5S1UfVNVkVU3BDZQ2zzeBAHiTA1W4EtcBb4wx9VJUVERcXJwlEI+IEBcX\n16w1s2a/TkREHgZWqOps4F4RuRI3h0IetYwCaowx1bEEcqrm3h/NkkRUdQFu/mtU9ec+yx/ETXPZ\n9HLSYfkLcNGvILR52gqNMaa1azvDnuRth2XPwoa3Ax2JMaaVOXDgADfeeCN9+/Zl9OjRTJgwgbff\nfpsFCxYQExPDyJEjGTx4MPfff3/lax566CEef/zxU8pJSUkhJ+cr06m0aG0nifS/EOIHwZKnweZQ\nMcb4iapy1VVXMXnyZHbs2MHKlSt57bXXyMzMBOCcc85h9erVrF69mvfff5/PP/+8jhKDS9tJIiEh\nMOEu2L8Odi4KdDTGmFZi3rx5tGvXjpkzZ1Yu6927N/fcc+okle3btyc1NZW9e1vXtPBBNwBjowyf\nAfN+BV88A33PDXQ0xhg/+uV7G9iYVeDXMs9IjOYXV5xZ6zobNmxg1KhRdZZ16NAhtm3bxuTJk/0V\nXovQdmoiAOGRMO4O2PYJHNwc6GiMMa3QXXfdxYgRIxg7diwAixcvZvjw4XTv3p3LL7+c7t27AzWf\nRRVsZ5u1rZoIwJhvw+InXG1k+jOBjsYY4yd11Riayplnnsmbb75Z+fhPf/oTOTk5jBkzBnB9Iu+/\n/z5bt25l0qRJXH311aSmphIXF8e+fftOKevIkSPExsY2a/yN1bZqIgAd4yD1Rlj3Ohw5EOhojDFB\nburUqRQVFfHss89WLjt27NhX1hs4cCAPPvggjz7qBuaYPHkys2fP5siRIwC89dZbjBgxgtDQ0OYJ\n3E/aXhIB18FeVgLL/xroSIwxQU5EeOedd1i4cCF9+vRh3Lhx3HrrrZXJwtfMmTNZtGgRO3fuZPjw\n4dx9991MmjSJ1NRUnnvuOV54IfgGPBcNstNdx4wZo36ZlOq1m2DX5/D9jdCuQ+PLM8Y0u02bNjFk\nyJBAh9HiVLdfRGSlqo7x97baZk0EYMLdcPwQrPlXoCMxxpig1XaTSK/xkDQalv4ZyssCHY0xxgSl\ntptERGDiPZC3A7Z8GOhojDEmKLXdJAIw+AqI7QVL7FRfY4xpiLadRELDYPxdsGcp7Fke6GiMMSbo\ntO0kAjDyZoiMgS+eDnQkxhgTdCyJRHSC0d+ETe9B3s5AR2OMCTKhoaGkpqYyYsQIRo0axZIlSwDI\nysri2muvBWDBggVcfvnlp7xu+vTpTJgw4ZRl77zzDhs3bqx8/OKLL5KVldXE/0HjWBIBOOu7IKGw\n9Nm61zXGGB/t27dnzZo1rF27lt/97nc8+KCbZy8xMZFZs2ZV+5r8/HxWrVpFfn4+O3ee/PFqSSRY\nRSfCsGth9Svu2hFjjGmAgoICOnfuDEBGRgZDhw6tdr0333yTK664guuvv57XXnsNgCVLljB79mx+\n9KMfkZqayqOPPsqKFSu46aabSE1N5fjx46SkpPCLX/yCUaNGMWzYMDZvDvxAsm1vAMaaTLgb1r4K\nK/4B5/wg0NEYY07XnAdg/3r/ltl9GEx7pNZVjh8/TmpqKkVFRezbt4958+bVWeyrr77KL37xC7p1\n68a1117Lgw8+yMSJE7nyyiu5/PLLK5vB5syZw+OPP145mCNAfHw8q1at4s9//jOPP/54wIdKsZpI\nhe5Doe95sOwvUFoc6GiMMUGiojlr8+bNfPTRR9xyyy3UNpzUgQMHSE9PZ9KkSQwcOJCwsDDS0tLq\nvb2vfe1rAIwePZqMjIzGht9oVhPxNfFueOUaSJvlRvo1xgSPOmoMzWHChAnk5OSQnZ1d4zqvv/46\nhw4dok+fPoBrAnvttdf49a9/Xa9tREREAK5Dv7S0tPFBN5LVRHz1Ox+6nuEuPgyygSmNMYG3efNm\nysrKiIuLq3GdV199lY8++oiMjAwyMjIq52QHiIqKqhwavrrHLZElEV8irm/k4AbYXne7pjHGVPSJ\npKamMmPGDF566aUa5wTJyMhg9+7djB8/vnJZnz59iI6OZtmyZVx//fU89thjjBw5ku3bt3Pbbbcx\nc+bMyo71lqjtDgVfk9IT8NRw6DoEbnmn6bZjjGk0Gwq+ejYUfCCFRcBZd8CO+bC//p1dxhjTFlkS\nqc7ob0J4B/jiT4GOxBhjWjRLItXp0AVGfgPW/wcK9gU6GmNMLYKtSb6pNff+sCRSk/F3gpbBl38J\ndCTGmBpERkaSm5tricSjquTm5hIZGdls27TrRGrSpQ8MuQJW/B3Oud8N1GiMaVGSk5PJzMys9bqM\ntiYyMpLk5ORm254lkdpMuAc2vuvG1Bo/M9DRGGOqCA8Pr7xozwSGNWfVpudY6DnezcNeFvgrQ40x\npqWxJFKXiXdD/i7Y/F6gIzHGmBbHkkhdBl0KnfvAkqdtKBRjjKmiyZOIiISKyGoReb+a5yJE5HUR\nSReRZSKS0tTxnLaQUJhwF+xdCbuXBjoaY4xpUZqjJvI/wKYanvs2cEhV+wNPAo82QzynL/UmaN8Z\nvngm0JEYY0yL0qRJRESSgcuAmmZNmQ685N2fBZwvItKUMTVIuw4w9juw+QPI3R7oaIwxpsVo6prI\nU8CPgfIank8C9gCoailwGPjKGMoicoeIrBCRFQE7H3zs7RAabkOhGGOMjyZLIiJyOXBQVVfWtlo1\ny77Se62qz6vqGFUdk5CQ4LcYT0tUNxh+Haz5NxzNDUwMxhjTwjRlTeRs4EoRyQBeA6aKyCtV1skE\negKISBgQA+Q1YUyNM+FuKD0OK/4W6EiMMaZFaLIkoqoPqmqyqqYA1wPzVPXmKqvNBm717l/rrdNy\nz6PtOgT6XwhfPg8lRYGOxhhjAq7ZrxMRkYdF5Erv4d+AOBFJB34APNDc8Zy2iffA0WxY93qgIzHG\nmICzmQ1Plyr85RwoLYbvLYUQu17TGNPy2cyGLYWIG5gxZwukzw10NMYYE1CWRBpi6NcgKhGW/DHQ\nkRhjTEC1mSSyM+coP3tnPcWlNV2ychpCw93Q8BmLIWtN48szxpgg1WaSSEbOUV5ZuptXv9ztnwJH\n3wbtomwoFGNMm9ZmksiUQQlM6BvHH/67jYKiksYXGBkDo26BtLfgcGbjyzPGmCDUZpKIiPCTS4eQ\nd7SYvyz00/hXFbMdLnvOP+UZY0yQaTNJBGBYcgzTUxN5YfFO9h0+3vgCY3vBmVfBypegqKDx5Rlj\nTJBpU0kE4P6LBqEKT3661T8FTrgbThTAqpf9U54xxgSRNpdEenbpwC0TejNrZSab9/uh9pA0CnpP\ngqXPQpkf+lqMMSaItLkkAnD31P50igjjkTmb/VPgxLuhIBM2vuuf8owxJki0ySQS26Edd53XnwVb\nsvk8PafxBQ64GOIGuIsPg2wYGWOMaYw2mUQAbp2YQlJse343ZxPl5Y384g8JcfOw71sLGZ/5J0Bj\njAkCbTaJRIaHcv/FA0nbW8B767IaX+CI66FDvF18aIxpU9psEgGYPiKJM3pE838fbeFEaVnjCgtv\nD+Nuh60fQfYW/wRojDEtXJtOIiEh7gLEvfnHeXnJrsYXOPY7EBZp87AbY9qMNp1EACYNiOfcgQk8\nPW8b+ceKG1dYx3gYcQOsfQ0KD/onQGOMacHafBIBeGDaYI6cKOXPC/wwHMqEu6DsBCx/ofFlGWNM\nC2dJBBjSI5prRiXz4ucZ7Mk71rjC4gfAwGkuiRQ3sixjjGnhLIl4fnDhQETgCX8MhzLxHjiWC2tf\nbXxZxhjTglkS8STGtudbk/rw9uq9pO093LjCek+ExJGug73cD5NgGWNMC2VJxMedU/rRuUM4v/1w\nE9qYK89FXG0kbztsneO/AI0xpoWxJOIjOjKce88fwJLtuSzcmt24woZMh5hesMQuPjTGtF6WRKq4\n6aze9OrSgUfmbKasMcOhhIa5Sat2L4HMlf4L0BhjWhBLIlW0Cwvhx5cMYvP+I7y1qpHT3o66BSJi\n4Iun/ROcMca0MJZEqnHZsB6M6BnL7z/ZyvHiRgyHEhEFo291Q8Qf8sMV8cYY08JYEqmGiPCTaYPZ\nX1DE3z/f2bjCzpoJEuImrTLGmFbGkkgNzuobxwVDuvHsgu3kFp5oeEExSTD0Gjd97vFD/gvQGGNa\nAEsitXhg2iCOFZfy9Lz0xhU04W4oOQorX/RLXMYY01JYEqlF/65RzBjbi1eW7iIj52jDC+oxHPqc\nC8v+AqWNHOTRGGNaEEsidfj+hQNoFxbCYx83co6QiffAkX2w4S3/BGaMMS2AJZE6dI2K5PZz+vLB\n+n2s3t2IPo3+F0DCYFjytM3DboxpNSyJ1MPtk/sS3ymC3324ueHDoYi4vpEDabBjgV/jM8aYQLEk\nUg+dIsK474IBfJmRx9xNjZhsavh10LErfPYElDdyOl5jjGkBmiyJiEikiHwpImtFZIOI/LKadW4T\nkWwRWePdvtNU8TTWjLE96ZvQkUfmbKK0rIEj84ZFwKTvw85F8NIVkL/Hv0EaY0wza8qayAlgqqqO\nAFKBS0RkfDXrva6qqd6txU4HGB4awgOXDGZ79lFeX9GIL//xd8JVz8G+tfDc2ZBmHe3GmODVZElE\nnULvYbh3C+oe5QvP6MbYlM48+ek2jp4obVghIpB6A8xcDHEDYNY34e074cQR/wZrjDHNoM4kIiJ3\ni0i0d/8vXhPV+fUpXERCRWQNcBD4VFWXVbPaNSKyTkRmiUjPGsq5Q0RWiMiK7OxGDtHeCCLCg5cO\nIafwBH9dvKNxhXXpC9/6CCb/CNa9Bs+dA5kr/BOoMcY0k/rURO5Q1QIRuQhIAu4E/q8+hatqmaqm\nAsnAOBEZWmWV94AUVR0OzAVeqqGc51V1jKqOSUhIqM+mm8yoXp25dFh3nl+0g4NHihpXWGg4TP0Z\n3PYBlJfC3y6CRY9Zp7sxJmjUJ4lUNEFNA/6hqivr+bqTBajmAwuAS6osz1XVioGp/gqMPp1yA+XH\nFw+muLScP8zd5p8Ce0+EmZ/BmVfBvF9bp7sxJmjUJxmsFZEPgSuAOSLSiXr0bYhIgojEevfbAxcA\nm6us08Pn4ZXApvoGHkgp8R25eXxvXlu+h/SDhXW/oD7ax8I1f7NOd2NMUKlPEvkm8BAwTlWPARHA\nt+vxuh7AfBFZByzH9Ym8LyIPi8iV3jr3eqf/rgXuBW473X8gUO6Z2p/24aE8+tHmuleuL+t0N8YE\nGanrCmzvtNx1qnpMRG4ARgJPq2pA2lvGjBmjK1a0jA7oP81P57GPt/DGdycwrk8X/xZeVgIL/w8W\nPw6xveGaFyB5jH+3YYxpM0Rkpar6/UukPjWR54HjIjIc+AlwAHjF34EEo2+d3Yfu0ZH89sNNDR8O\npSah4TD1p9bpboxp0eqTRErVfUNOB/6gqr8Hopo2rODQvl0oP7hoIGv25DMnbX/TbMQ63Y0xLVh9\nkshREfkR8A3gAxEJwV04aIBrRiUzqFsUj360meLSBg6HUhfrdDfGtFD1SSIzAAG+q6r7cNd8PNGk\nUQWR0BDhgUsHsyv3GP9etqvpNmSd7saYFqjOJKKqWcDfgQgRuQQ4pqr/aPLIgsiUgQlM7BfHH+el\nU1BU0rQbq7zS/cd2pbsxJuDqM+zJNcAqXHPWLcAKEbm6qQMLJiLCTy4dQt7RYv6ycHvTb9A63Y0x\nLUR9mrN+DoxV1ZtU9UbgLNx1I8bH0KQYrkpN5IXFO9l3+HjzbLRqp/uLl1unuzGmWdUniYSo6gGf\nx9n1fF2b88OLBqEKT3yytfk26tvpvn8dPHs2pL3ZfNs3xrRp9UkGn4jIhyJys4jcDMwGPm7iuIJS\nzy4duHVib2atymTz/oLm27Bvp3v8AJj1Let0N8Y0i/okkftxo+uOwzVlvQT8qCmDCmZ3ndefqIgw\nHpnjx+FQ6ss63Y0xzaw+Z2epqr6uqveq6j2q+h/1++XZrUdsh3bcM3UAC7Zk83l6TvMHYJ3uxphm\nVGMSEZFDIpJXze2QiOQ1Z5DB5hsTepMU257fzdlEeXmA8q11uhtjmkFtNZF4IKGaW8VyU4PI8FB+\ndPEg0vYWMHttVuACsU53Y0wTqzGJeLMS1nhrziCD0ZUjEhmaFM1jH2+hqCSAu8u30z1hoHW6G2P8\nyk7VbSIhIcJPpg1hb/5x/vlFEw6HUl9d+sI355za6b7ri0BHZYwJcpZEmtDE/vFMGZTA0/O2kX+s\nONDhfLXT/R+XwMtXQcbngY7MGBOkLIk0sQemDebIiVL+vKAZhkOpr94T4XtL4cKH4UAavHgp/P0S\n2DYX7MQ7Y8xpsLOzmtjg7tFcOyqZFz/PYE/esUCHc1JEJzj7f+C+9TDtMcjfDf+6Bp6fApveh/Im\nGtbeGNOq2NlZzeAHFw0kJAR+/8mWQIfyVeHt4aw74N41cOXTUHQYXr8Jnp0I62fZ9SXGmFrV++ws\nIAbo5nMz9dQjpj3fntSHd9Zkkbb3cKDDqV5YOxh1C9y9Ar72AqDw5rfhmTGw6p9Q2gL6dIwxLU59\nhoK/TES2ApnAMu/vvKYOrLX57rn96NKxXdPMx+5PoWEw/Otw5xcw4xWIiILZd8MfR8KXf4WSZhqh\n2BgTFOrTsf4b4Gxgi6r2BC4GFjRlUK1RdGQ4907tz5LtuSzcmh3ocOoWEgJDroA7FsJNsyAmGT68\nH/4wAj7/I5woDHSExpgWoD5JpFRVs4EQERFV/RQY1cRxtUo3ntWb3nEdePj9jWQeakGd7LURgQEX\nuoEdb/sAug6BT/8fPDUUFv4fHM8PdITGmACqTxI5LCIdgc+Al0Xk94CdutMA7cJC+O3VwzhYcIJp\nTy3mrVWZLbtpy5cIpEyCW96F7/wXeo6H+b+Bp4bB3F/C0QAMNmmMCTip60tMRKKAY7iEcwuug/1l\nVQ3It8aYMWN0xYrgHt58T94xfvDGGpZnHOKyYT34zdVDie3QLtBhnb7962Hx72HDO+4sr9G3wcR7\nIDox0JEZY6oQkZWqOsbv5dYjifxWVX9S17Lm0hqSCEBZufKXRdt58tOtdOnYjse/PoJzBgTpmdPZ\nW+GzJ2DdGxASCiNvhrPvg869Ax2ZMcYTyCSySlVHVVm2VlVH+DuY+mgtSaRC2t7D3Pf6GtIPFnLb\nxBQemDaYyPDQQIfVMIcy4LOnYM2/3PUlw2fAOT9wsy0aYwKq2ZOIiHwXmAkMBHyvkosCVqjqDf4O\npj5aWxIBKCop45E5m3lxSQYDunbiyRmpDE2KCXRYDVeQBUuehhX/gNIiN6fJOT+E7sMCHZkxbVYg\nkkhnIA74HfCAz1NHVPWgvwOpr9aYRCos3pbN/f9ZS97RYu67YCAzz+1HaIgEOqyGK8yGpX9215cU\nH4GB02Dy/ZDs9+PYGFOHgDVneRsfCkzyHi5W1Q3+DqS+WnMSAcg/VsxP307jg/X7GJvSmSeuS6Vn\nlw6BDqtxjh9yiWTpn939vlNg8o+g99nurC9jTJNrqiRSnyvW7wLeAHp5tzdE5Hv+DsQ4sR3a8cyN\nI3lyxgg27zvCtD8sZtbKIDoVuDrtO8O5P4b70uDCX8GBjfDiZTZysDGtQH061tcBE1W10HvcCVii\nqsObIb6vaO01EV+Zh47xwzfWsmxnHtOGduc3Vw+jS8cgPBW4qpLjsPoV1wlfkAnxg2DkTa4jPqp7\noKMzplUK5NlZ64ExqnrCexyB61gPSC9pW0oi4E4FfmHxDh7/ZAuxHdrx2LXDmTKoa6DD8o/SYlj/\nH1j1EuxZBhIK/S+A1Bth0DQIiwh0hMa0GoHoWA9T1VIR+TFwA/Cm99TVwKuq+nitBYtEAouACCAM\nmKWqv6iyTgTwMjAayAVmqGpGbeW2tSRSYWNWAfe9vpqtBwq5ZUJvHpw2hPbtgvRU4OrkpMPaf8Oa\nV+FIlmsCG/Z1l1B6pFrfiTHZG6pLAAAgAElEQVSNFIgkUnl9iIiMBc4BBFikqsvrLFhEgI6qWigi\n4bhhU/5HVZf6rPM9YLiqzhSR64GrVXVGbeW21SQC7lTgxz7ewt8+20m/hI48NWMkw5KD+FTg6pSX\nwY4FsObfsOk9KDsBXc+A1Jtg+HXQqZXUwoxpZoFIIqtVdaRfNiLSAZdE7lTVZT7LPwYeUtUvRCQM\n2A8kaC1tbG05iVT4PD2HH76xlpzCE9x3wQBmntuPsNBWONPx8XzY8Bas/hfsXeGauwZc5PpPBlzs\n5kAxxtRLIJJIJvBETS9U1Rqf8ykjFFgJ9Af+pKr/W+X5NOASVc30Hm8Hzqo6LpeI3AHcAdCrV6/R\nu3btqmvTrd7hYyX87N003lubxejenXnyulR6xQX5qcC1yd7iaidrX4PC/dAhDoZd5zV3BeQcD2OC\nSiCSyD7gWVwT1leo6i/rvRGRWOBt4B5VTfNZvgG4uEoSGaequTWVZTWRU727Zi8/eyeN8nLlF1ec\nydfHJCOtuf+grBR2zHdDq2z+AMqKodswVzsZ9nXoGB/oCI1pkQLaJ+KXDYn8Ajjq2yFvzVn+sTf/\nOPe/sZYvduRy0Rnd+N3XhhHXqQ2c2XQsD9LedAklazWEhMHAS1z/yYALITQ80BEa02IE4mLDRv2c\nFZEErwaCiLQHLgA2V1ltNnCrd/9aYF5tCcRULym2Pf/6zln89NIhLNiSzcVPLWb+5oCNTNN8OnSB\ncbfDHQvcdL7j74Q9X8JrN8ATQ+Djn8KBgA2uYEybUFtNpIuq5jW4YJHhwEtAKC5ZvaGqD4vIw7jr\nTGZ7pwH/ExgJ5AHXq+qO2sq1mkjtNu0r4Puvr2Hz/iPcdFYvfnrZEDq0Cwt0WM2nrATS/+tqJ1vm\nQHkJ9BgBqTfDsGtd4jGmDQro2FktiSWRuhWVlPHEp1v56+Id9InryJMzUhnRMzbQYTW/o7mQNstd\nHb9/HYSEu4sYR94M/c6H0DaUXE2bZ0nEY0mk/r7YnssP31jDgSMnuHfqAO46r5WeClwf+9e7CxnX\nvQ7HcqBTNzfMSupN0HVwoKMzpslZEvFYEjk9h4+X8It303hnTRYje8Xy5HWppMR3DHRYgVNaDOmf\numtPtn0M5aWQOMqdKjzsWnelvDGtkCURjyWRhnlvbRY/fXs9peXK/7v8DK4f27N1nwpcH4XZbuyu\nNf+CA2kQGgFDrnDNXX3OhZA2WmszrZIlEY8lkYbbd/g49/9nLZ+n53LBkG48cs0w4tvCqcB1UYV9\na10yWfcGFOVDTC9XO0m90eaKN62CJRGPJZHGKS9X/rEkg0c/2kxEaAjfnNSHb0/qQ0x7u6YCgJIi\n2PKB64zfPh9QVysZ+Q0YcjmEtw90hMY0iCURjyUR/0g/eITff7KVOWn7iYoM49uT+vCtSX2IjrRk\nUil/D6x9FVb/E/J3Q0SM6zcZeTMkjrSRhU1QsSTisSTiXxuyDvOHudv4ZOMBoiPDuP2cvtx2dgpR\nlkxOKi+HXZ/Bqn/CptlQWgTdhrpkMuw66BgX6AiNqZMlEY8lkaaRtvcwT83dxtxNB4jtEM7t5/Tl\n1okpdIqwaylOcTzfDbWy+hXIWuWuPRl8qWvu6jcVQlrRHC+mVbEk4rEk0rTWZebz1NxtzNt8kM4d\nwrl9cl9unZBCR0smX3VggztVeN1rcCwXohIh9QZ37Ulcv0BHZ8wpLIl4LIk0jzV78nlq7lYWbMmm\nS8d2fHdyX74xoXfbGkKlvkqLYetHrnaS/iloOfQ+2zV3nTEd2rXh63JMi2FJxGNJpHmt2n2Ip+Zu\nY9HWbOI7tWPmuf246azerWtqXn8q2Od1xr8CeduhXScY+jXX3JU81jrjTcBYEvFYEgmMlbvyePLT\nbXyWnkN8pwjunNKPm87qRWS4JZNqqcLupe7Mrg1vQ8kxiB/oaifDr4eoboGO0LQxlkQ8lkQCa3lG\nHk9+upUl23PpGuWSyQ3jLJnU6sQRl0hWvwJ7lrlpfgde7BLKgIts3hPTLCyJeCyJtAxLd+Ty1Nyt\nLN2RR7foCO46rz8zxvYkIsySSa2yt8KaV9xgkEcPQscEGHG9a+5KGBTo6EwrZknEY0mkZVmyPYen\nPt3Glxl59IiJ5Hvn9ee6McmWTOpSVgLpc13tZOtHbiDI5HFumt8zpttAkMbvLIl4LIm0PKrKku25\nPPnpVlbsOkRiTCR3Te3P10f3pF2YDWJYp8KDboj6Vf+EnC0Q2g76XwBDr3Hzn9jZXcYPLIl4LIm0\nXKrK4m05PDl3K6t355MU2557pvbnmtHJhLfVeUxOh6q7gHH9m7DhLTiyD8I7ukQy7Fo3kVZYu0BH\naYKUJRGPJZGWT1VZuDWbJ+duY+2efHp2ac895w3g6lFJlkzqq7wMdi1xMzNufBeOH4LIWDjjShh6\nLaRMsqvjzWmxJOKxJBI8VJUFW7J5cu5W1mUepleXDtwztT9Xj0xquzMsNkRpMeyYD+tnweYPoOQo\ndOoOZ17taihJo+36E1MnSyIeSyLBR1X576aDPDl3KxuyCkiJ68C95w/gyhGJlkxOV/ExNyPj+lmw\n7RMoK4bOKa7/ZOi10O2MQEdoWihLIh5LIsFLVfl04wGemruNjfsK6BvfkXvPH8AVIxIJDbFf0qet\n6DBset81ee1YCFoGXc9wV8gPvRa69Al0hKYFsSTisSQS/MrLlU82HuCpuVvZvP8I3aMjuWJED6an\nJnFmYrRN29sQhdmw8R1XQ9mz1C1LGu2SydCvQVT3wMZnAs6SiMeSSOtRXq58uukA/1mRycKtBykp\nU/omdOSq1CSuHJFISryd2tog+bsh7S1XQ9m/HhDXET/sWhhyJXToEugITQBYEvFYEmmd8o8V8+H6\n/by7Zi/LduYBMKJnLFelJnLZ8B50jYoMcIRBKnurm/8kbRbkprv5T/qf72oog6ZBRKdAR2iaiSUR\njyWR1i8r/zjvr8vindVZbNxXQIjA2f3jmZ6axMVndrNZFxtCFfatdckk7S0o2Ath7U9eg9L/AgiL\nCHSUpglZEvFYEmlbth04wuy1Wby7JovdecdoFxbCBUO6Mj01iSmDEmx4lYYoL3f9JutnuX6UY7lu\n/vghV8CwayBlMoTavDGtjSURjyWRtklVWb0nn9lrsnh/XRY5hcVERYZx6dAeTE9N5Ky+cXaGV0OU\nlbgzu9JmuTO9io+4QSHPuAoGX+Ym17Kr5FsFSyIeSyKmtKycz7fn8u6avXyctp+jxWV0i47giuGJ\nTE9NYmiSneHVICVF7tqTtFmw9WMoLYKIaNeHMuhS1+RlnfJBy5KIx5KI8XW8uIz/bj7Au2uyWLDl\n5Ble00ckMT3VzvBqsOJjsHMhbPkQtnzkhq2XUOg1wfWjDJpm88gHGUsiHksipib5x4qZk3byDC9V\nd4bX9BGJXD7CzvBqsPJyyFrtJZQ5cHCDWx4/CAZd4mopyWNtLK8WzpKIx5KIqY99h4/zntchvyHr\n5BleV45I5OKh3Ym2M7wa7tAuNwfKlg8h4zM3F0qHOBh4ibv1m2qnDrdAlkQ8lkTM6Uo/eIR315x6\nhtf5g0+e4WVT+zZC0WFI/6+roWz72D0ObQd9znVNXgMvgZikQEdpCMIkIiI9gZeB7kA58Lyq/qHK\nOlOAd4Gd3qK3VPXh2sq1JGIaSlVZsyefd6uc4TVtaHeuSk1iXJ8uNiBkY5SVwO6lrpay+QM45H2s\ne4xwTV4DL3H37aSHgAjGJNID6KGqq0QkClgJXKWqG33WmQLcr6qX17dcSyLGH6o7wys6MozJAxOY\nMqgr5w5MICHKLr5rMFXI2XqyY37PMkAhOsklk0GXQp9z7ALHZhR0SeQrGxJ5F3hGVT/1WTYFSyIm\nwIpKypi/+SDzNh9kwdZsso+cAGBYUgznDUrg3EFdSe0Za9ehNMbRHHfa8NY5kD7PzYkS3hH6T3UJ\nZcBF0DE+0FG2akGdREQkBVgEDFXVAp/lU4A3gUwgC5dQNtRWliUR05TKy5WN+wpYsOUgC7Zks2r3\nIcoVYjuEM3lAAucNTmDygATiOtkv6AYrKYKMxSdrKUeyQEIgeZx3+vClED/Amr38LGiTiIh0AhYC\nv1HVt6o8Fw2Uq2qhiFwK/EFVB1RTxh3AHQC9evUavWvXriaN2ZgK+ceKWbwth/lbDrJoazY5hcWI\nwPDkWKYMTOC8wV0ZnhRDiNVSGqZiTK8tc1xS2b/OLe/Sz+uYvxh6nmXNXn4QlElERMKB94GPVfWJ\neqyfAYxR1Zya1rGaiAmU8nIlLeswC7ZkM3/LQdbsyUcVunRsx7kDE5gyyNVSOne0YUIa7HCmd/rw\nHNi5yM3cGNYeek9wZ3z1nQLdh0OInQBxuoIuiYgbd+IlIE9V76thne7AAVVVERkHzAJ6ay1BWRIx\nLUXe0WIWb8tmwZZsFm7NJu9oMSHiLnA8b1BXpgxKYGii1VIa7MQRyPgcdixwt+xNbnn7Lq5Tvu8U\nl1i69LWmr3oIxiQyCVgMrMed4gvwE6AXgKo+JyJ3A3cCpcBx4AequqS2ci2JmJaorFxZv/cw873O\n+XWZrpYS36kd5w7sWllLielgFzk22JH9rnZSkVQK9rrlMb2gr1dL6TMZOnUNXIwtWNAlkaZiScQE\ng9zCEyzals38zdks2pZN/rESQgRG9erMeYPdKcQ2FXAjqELudtgx343xtXORu9ARoNvQk01fvSfa\n1fMeSyIeSyIm2JSVu4scF245yPwt2azf677sukZFcK7XOT9pQLwNxdIY5WWwb40b1n7HAnfRY9kJ\nCAlz43r1neISS/IYCG2b+9mSiMeSiAl22UdOsHBrNgu8M74KikoJDRFG9+7MlEEJTOofz5mJMXZd\nSmOUHHcXOO5Y4BJL1mpAoV0nN0dKRfNX1zPaTH+KJRGPJRHTmpSWlbNmTz7zvetSNmS5y6iiIsMY\n3zeOs/vFMbF/PAO6drKmr8Y4lucGi9zp1VRy093yjl1dP0rfKS6xxPYKYJBNy5KIx5KIac0OHili\n6Y48vtiew+fpuezOOwa4DvoJ/eKZ2C+Os/vF07NLe0sqjXE482TT186FUHjALe/S92TTV5/JrWoS\nLksiHksipi3Zk3eML3bk8sX2XD5Pz+GgNyRLUmx7JvaLY2L/OCb0jad7jM2V0mCqkL35ZNNXxmdu\nmmAEegx3SSXlHNef0r5zYGNtBEsiHksipq1SVbZnH+WL7Tks2Z7LFztyyT9WAkDfhI6VtZTxfePs\ngsfGKCuBvatONn3t+RLK3X4mYbDrqO95lrvF9Q+aCx8tiXgsiRjjlJcrm/YXsCQ9lyXbc/hyZx5H\ni8sQgSHdoytrKuP6xNEpIizQ4Qav4qOwd6XrqN+z3P0tynfPRcZCz3HuljwOkka32FOKLYl4LIkY\nU72SsnLWZR6u7E9ZufsQxaXlhIYII5JjmOj1qYzq3dkm4mqM8nLXMb9nGWR+6Woq2ZvdcxLirlPp\nedbJ5BLbu0WcAWZJxGNJxJj6KSopY9WuQyzZnsvn23NYl3mYsnKlXVgIY3p3ZmK/OCb0i2d4cgzh\nNhlX4xw/BJkVtZVlruZSXOie69TtZE2l51luYq7w5u/DsiTisSRiTMMcKSpheUYeS9Jz+Xx7Lpv2\nudOJO7YLZVyfLpzdP54J/eIY0j3axvtqrPIyOLjRSypfur+HMtxzoe1cIqmorSSPg+geTR6SJRGP\nJRFj/CPvaDFLd7j+lCXbc9mRfRRwc6eM7xPH2D5dGJvSmSE9oq2m4g+FB08mlMzlrvO+zJ1tR0yv\nk81fPce5JjE/X1lvScRjScSYprH/cFFlQlm6I5fMQ8cBaB8eyshesYxJcUllZK/O1lHvD6XFbv4U\n39rKkX3uufAOrpO+8kywcY2+ZsWSiMeSiDHNY//hIlbsymNFxiGWZ+SxaV8B5QohAkN6RDM2pQuj\ne3dmbEoXu07FH1TdRZAVNZU9y2DfOtAy93xcfxh/J4z9ToOKtyTisSRiTGAUnihl9e5DLM84xIqM\nPFbvzud4ifuCS+7c/pSkMqBrJ+tX8YfiY27cr4rayuDLYNQ3GlSUJRGPJRFjWoaSsnI27SuoTCrL\nMw6RU+ja+KMjwxjjk1SGJ8fYacUBZknEY0nEmJZJVdmdd8wnqeSx3eusbxcawrDkGMb07lyZXLrY\nVfXNypKIx5KIMcEj72gxK3edTCrr9x6mpMx95/Tv2qkyqYxN6UyvLh1sUMkmZEnEY0nEmOBVVFLG\nuszDLM/Iq0wuBUWlACRERZySVM7oEU2YnVrsN02VROw8PWNMs4kMdxc2juvjTlctL1e2HSxkeUYe\nKzLyWLHrEHPS9gPQoV0ow5NjGJoYw5lJ0QxNjKFvQiebrKuFsZqIMaZF2Xf4OCu8fpU1mYfZvK+A\nE6XlgLtmZXCPKJdYEqMZmhTDgG6diAizTvu6WHOWx5KIMW1LaVk527OPkrb3MBuyCkjLOszGrAIK\nT7hmsPBQYUDXKIYmRXNmYgxDk6IZ0iOaDu2socWXJRGPJRFjTHm5OxMsLctLLF6CyTtaDLhBc/vG\nd2RokldjSYzhzMQYYjr4dyiRYGJ9IsYY4wkJEVLiO5IS35HLhycC7hTj/QVFpO0tYEPWYdL2FrB8\nZx7vrsmqfF1y5/anNIWdmRRN1yi72r4xLIkYY1oFEaFHTHt6xLTnwjO6VS7PLTzBhqyCU5rCPtqw\nv/L5hKgIhiaebAo7MzGG5M42h319WRIxxrRqcZ0imDwwgckDEyqXHSkqYdO+I6TtPVyZWBZty6Gs\n3DXvx7QP58zE6Moay+Du0aTEd7AO/GpYEjHGtDlRkeGnnGoM7hqWLfuPkOY1hW3MOsxLX+yi2Dsz\nLESgd1xH+iV0on/XTvRL6Ej/ru5+VGTb7WuxJGKMMbhrWEb0jGVEz9jKZSVl5aQfLGTrgSNsP1hI\nenYh6QcLWbj1YOWV9wDdoiNcQqlIMF5ySegU0eqbxSyJGGNMDcJDQxjSw50y7Ku0rJzdecdI90ks\n2w8WMmtlJkeLyyrXi44Mq6ytVN4Sokjq3L7VXDRpScQYY05TWGgIfRM60TehExf5LK84Qyz9YOEp\nt3mbD/LGiszK9SLC3Ot9ay/9u3YKyn4XSyLGGOMnvmeInTMg4ZTn8o8Vn5pcsgtZvfsQ7609eQpy\nMPa7WBIxxphmENuhHWNSujAm5dRpbo8Xl7E9u5Dt2afWXqrrd7n9nL5855y+zR16rSyJGGNMALVv\nF8rQpBiGJsWcsrzEt9/F63NJiIoIUJQ1syRijDEtUHhoCP0SOtEvoRMXnxnoaGrWZIP1i0hPEZkv\nIptEZIOI/E8164iI/FFE0kVknYiMaqp4jDHG+F9T1kRKgR+q6ioRiQJWisinqrrRZ51pwADvdhbw\nrPfXGGNMEGiymoiq7lPVVd79I8AmIKnKatOBl9VZCsSKSI+miskYY4x/NcvckyKSAowEllV5KgnY\n4/M4k68mGkTkDhFZISIrsrOzmypMY4wxp6nJk4iIdALeBO5T1YKqT1fzkq9McKKqz6vqGFUdk5CQ\nUM1LjDHGBEKTJhERCcclkH+p6lvVrJIJ9PR5nAxkVbOeMcaYFqgpz84S4G/AJlV9oobVZgO3eGdp\njQcOq+q+porJGGOMfzXl2VlnA98A1ovIGm/ZT4BeAKr6HPAhcCmQDhwDvtmE8RhjjPGzoJtjXUSy\ngV0NfHk8kOPHcIKd7Y9T2f44yfbFqVrD/uitqn7vVA66JNIYIrKiKSaqD1a2P05l++Mk2xensv1R\ns2Y5xdcYY0zrZEnEGGNMg7W1JPJ8oANoYWx/nMr2x0m2L05l+6MGbapPxBhjjH+1tZqIMcYYP7Ik\nYowxpsHaTBIRkUtEZIs3d8kDgY4nkOoz10tbIyKhIrJaRN4PdCyBJiKxIjJLRDZ7x8iEQMcUKCLy\nfe8zkiYir4pIZKBjamnaRBIRkVDgT7j5S84AbhCRMwIbVUBVzPUyBBgP3NXG9wfA/+CmKzDwB+Aj\nVR0MjKCN7hcRSQLuBcao6lAgFLg+sFG1PG0iiQDjgHRV3aGqxcBruLlM2qR6zvXSZohIMnAZ8EKg\nYwk0EYkGJuPGvUNVi1U1P7BRBVQY0F5EwoAO2ACxX9FWkki95i1pi2qZ66UteQr4MVAe6EBagL5A\nNvAPr3nvBRHpGOigAkFV9wKPA7uBfbgBYj8JbFQtT1tJIvWat6StqWOulzZBRC4HDqrqykDH0kKE\nAaOAZ1V1JHAUaJN9iCLSGddi0QdIBDqKyM2BjarlaStJxOYtqaIec720FWcDV4pIBq6Zc6qIvBLY\nkAIqE8hU1Yqa6SxcUmmLLgB2qmq2qpYAbwETAxxTi9NWkshyYICI9BGRdrjOsdkBjilg6jnXS5ug\nqg+qarKqpuCOi3mq2mZ/barqfmCPiAzyFp0PbAxgSIG0GxgvIh28z8z5tNGTDGrTlPOJtBiqWioi\ndwMf486w+LuqbghwWIFU7VwvqvphAGMyLcc9wL+8H1w7aKPz/KjqMhGZBazCndG4Ghv+5Cts2BNj\njDEN1laas4wxxjQBSyLGGGMazJKIMcaYBrMkYowxpsEsiRhjjGmwgCURESn0/qaIyI1+LvsnVR4v\n8Wf5VcqOEJG5IrJGRGY0sIyHROSYiHT1WVboc19F5J8+j8NEJFtE3heRb3rbXiMixSKy3rv/yGnG\nkOidzoiIpIrIpVXiu78eZXQSkWdFZLs3ZMZKEbndey5FRI57sW0UkZe9Cx4RkdtE5JkqZS0QkTGn\n8z/4m4hMCfSovr7Hgc+yWBH5ns/jRsXZio6/DJ/trxeR6T7PLfH+pohImnd/jIj88XTi9BcRWSsi\nr1ZZdpuIJPo8vk9EOjRiG1NEZKLP45kicktDy6tJS6iJpACnlUTEjcpbm1OSiKo25VWmI4FwVU1V\n1dfr84Ia4s8BfljDS44CQ0Wkvff4QmAvgKr+w9t2Ku4q/PO8x6c1VIWqZqnqtd7DVODS2tavwQvA\nIWCAN2TGJUAXn+e3e3EOw40acF0DttFieYP0NYdY4Ht1rnV6WsPxR8X2gWuBygRR3XeAqq5Q1Xsb\nuJ16qe6zLiJDcN+9k6uMS3YbbniVCvfhBn1sqCn4XGGvqs+p6suNKK9aLSGJPAKc4/16+L64eR0e\nE5HlIrJORL4LlVl1voj8G1jvLXvH+7W7QUTu8JY9ght1c42I/MtbVlHrEa/sNO+XygyfshfIyTkU\n/iUiUlGe98t5nYg87hu498vtFSDV214/ETnf+xW+XkT+LiIR3roZIvJzEfkM+Ho1++HvwAwR6VLN\ncwBzcCPNAtwAvFrDetUSkQ9FZLh3f7WI/Ny7/ysR+U7FLzRxF5g97MXiW7s6w9tHO0TkKx88EemH\nGy35Z6paDuANF/Fo1XVVtQz4kgYMgikio0Vkofe+fywiPbzlC0TkURH5UkS2isg53vJQEXncez/W\nicg93vKa3qdLvGPgM+BrPtvt6K233HvddG/5bSLyHxF5D/jK4HzVHaPe8kIR+Y33i3SpiHTzlvcR\nkS+87fyqht3wCNDPe38e85Z1quH4rXZ/VSOoj79qRON+0FRsv7oaXWUNTlxt5+/VbUNEbvaOqzUi\n8hfxEoO4WvcK7739pc/6dX3WbwT+iTtervRecy0wBneR5xpxc/wkAvNFZL63zkXesbHKO+Y6+Wzv\nl97y9SIyWNzAqjOB73vlnSM+NTpxtb2l3mfibXHjhNX4OaqVqgbkBhR6f6cA7/ssvwP3RQQQAazA\nDYA2BfeLqI/Pul28v+2BNCDOt+xqtnUN8CnuqvVuuGENenhlH8b9Og4BvgAm4X5Fb+HkRZmx1fwf\nlfEDkbjRggd6j1/GDW4IkAH8uIZ98RBwP/Bz4JdV/wegEBiOG8coElhTdb/5bCO+hm08ANyF+3At\nBz72ls8HBuFqhGnestuAZ6rEt8R7P+KBXFzty7f8K4G3a3m/fcuP9LY7vLrtecsW4OZx8F0W7sWR\n4D2egRt9oGL933v3LwXmevfvxI0RFlZxzNT0PvksH4AbtPMNn/f2t8DNFccBsBXo6MWeiXcsVvN/\n13SMKnCFd///OHnMzwZu8e7fRZVjueq+9DkGqzt+a9xfre3489n+em8/HwMur+Y7wHc7lf9DTdsA\nhgDvVWwP+LPP+1Px3obijr/hPnFU+1n3nt8K9AYuAmbXdMz77k8vpkVAR+/x/wI/91nvHu/+94AX\nfN/Xqu+zd38dcK53/2Hgqdo+R7XdWkJNpKqLgFvEDcexDIjDfagBvlTVnT7r3isia4GluAEWB1C7\nScCrqlqmqgeAhcBYn7Iz1f2KXoM72AqAIuAFEfka7sCszSDcgG1bvccv4eZmqFBXc9cfgVvFzelw\nClVd58V0A9CQ4UkWe7FMAj7A/XLtAKSo6pZ6vP4DVT2hqjnAQVwSrpGI/NT7BeQ70GU/733NBXZ7\n/xPUPKJy1eWDgKHAp145P8N9cVaoGEhyJW5fgRtE7zlVLQVQ1Txqfp8Ge8u3qfsU+Q7EeBHwgLfd\nBbgv017ec5965VanpmO0GKjox/CN92xO/sqv7Ieoh+qO37r2V1Wt4fg7T90EUsOAZyp+rddTdds4\nHxgNLPf24fm44fIBrhORVbjhUM7ETXhXodrPuoiMBbJVdRfwX2BURS2gDuO98j/34rgVl4gqVHfs\nV0tEYnA/iBd6i6p+T9W7LGiZY2cJLqt+fMpCkSm4mojv4wuACap6TEQW4D7YdZVdkxM+98twv1xL\nRWQc7sC5HrgbmNrA8sEn/uqoar645rqa2rtn4+Y3mIJLrqdjOa66vANXG4sHbscdKPXxlf1T5fmN\nwAgRCVHVclX9DfCbKs0I21U11WtSWSAiV6rqbFxSqfpB6oJrp/clwAZVrWm61ooYfeMTvpqManuf\nakpoAlxT9QtPRM6ihve1jmO0xEtUVeOtLYbaVPf+1LW/ThHkx98pVHW7iBzAffF+2YhtCPCSqj7o\nu6KI9MHV3saq6iERea2Fw/sAAAMASURBVJFTv39q+qzfAAwWN2o0uJrZNdQ9IZrgfqzcUEfsde6b\nejitslpCTeQIEOXz+GPgTjl55s5AqX5SnBjgkPfhHIzL1BVKKl5fxSJcW2uoiCTgsm+NB5j3KyZG\n3cCE9+E6/GqzGUgRkf7e42/gajun4wngu1T/5v0deFhV159mmaib0XEPrjN7Ke6X4f3e36qqvif1\nKT8d1/T4a58240iq+cJW1X245o2KD+Zy4GwR6e69bgyuWWFPlZduARLEm/NbRMJF5Mw6QvsEmCle\np7fX5l/T+7QZ6COufwfcB77Cx8A9IpV9DSPr2C7UfozW5HNOTsF6Uw3r1Pf9acj+Csrjrypx/ZV9\ngF2NKQdXW7jWKw8R6SIivXFf/keBw+L6s6bVI6YQXB/JcFVNUTdy9HROHmdV/2/fx0txn5H+Xlkd\nRGRgHZusdj+q6mHgkE9/R0O+pyq1hCSyDigV18H4fVxG3gisEncq3l+o/oD+CAgTkXXAr3A7ucLz\nwDrxOtZ9vO1tby0wD9duub+W2KKA971tLAS+X9s/oqpFuBFP/yMi63Ez5T1X22uqKSPHizOimucy\nVfUPp1NeFYuBA6p6zLufTPUf4vm4jszTPW35O7hfqOkishKYi2u7rc47/P/27h4lgiAIoPCbUG+w\nt/ACegfBRBMz003NjcTE0MDAzAOIChoIgiCYrIt6AyPxDGVQtbqos2pHLrwvnJ9merrpYqpgGha7\nrluu1OIQOKtP9X1gvVIz72ohWgN2K0U04uf9HQ7J2te47tnoG6c6vgWcVlF0egHaIXPk45qXfUXv\nabPmaJ8huef9HRmEvoiIVzKt8dB9FNa/u+7P72vO5x9kIXpUbWzX3GoWEU9kGvCixvESGETEPZnG\neiSD680vmlsBniN3TJy4Jvs6AI6Ag+r3ArmOnXdddxURL2St6Lie45ZMv85yAqxOCuufzm0Ce9XW\nElkXaeJffCVJzf7Dl4gkaU4ZRCRJzQwikqRmBhFJUjODiCSpmUFEktTMICJJavYGPAc/oJowxuYA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc68e80e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with open('loss_gru_9.txt','rb') as file:\n",
    "    losses_gru = pickle.load(file)\n",
    "        \n",
    "with open('loss_bil_9.txt','rb') as file:\n",
    "    losses_bil = pickle.load(file)\n",
    "    \n",
    "iteration= list(range(10))\n",
    "\n",
    "plt.plot(iteration, losses_gru, label ='GRU')\n",
    "plt.plot(iteration, losses_bil, label = 'BilAttn')\n",
    "plt.xlabel(\"Iterations for NMT with GRU encoder and the NMT with Bilinear Attention\")\n",
    "plt.ylabel('Total loss')\n",
    "plt.legend()\n",
    "plt.title('Loss as a function of the iteration')\n",
    "plt.savefig(\"mt_gru_bil.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    \n",
    "    print(e)\n",
    "    with open('model_NMT_bil_' + str(portion) + '_'+str(e)+'.pickle','rb') as file:\n",
    "        model_NMT = pickle.load(file)\n",
    "        \n",
    "        write_test_eval(model_NMT, test_corpus2id_fr, test_corpus2id_en, str(portion), str(e), 'bil')\n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of sentences that were not added is 0\n"
     ]
    }
   ],
   "source": [
    "#get val sentences \n",
    "\n",
    "val_tokens_list_en,val_sentence_list_en = tokens_sentences(val_en)\n",
    "val_tokens_list_fr,val_sentence_list_fr = tokens_sentences(val_fr)\n",
    "\n",
    "val_corpus2id_en, val_corpus2id_fr = convert_corpora2id_both(val_sentence_list_en,val_sentence_list_fr, tokens2id_en, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the results of the predicted sentences to a txt file for evaluation\n",
    "\n",
    "def write_val_eval(model_NMT, val_sentences_fr, val_sentences_en, portion, e, label):\n",
    "\n",
    "    filename = 'val_results_' + portion + '_' + e + '_' + label + '.txt'\n",
    "    output = open(filename,\"w\") \n",
    "    \n",
    "    for sent in range(len(val_sentences_fr)):\n",
    "\n",
    "        decoder_outputs, decoder_attentions = evaluate_sent(model_NMT, val_sentences_fr[sent], val_sentences_en[sent])\n",
    "        \n",
    "        output_list = word_ids2string(decoder_outputs, id2tokens_en)\n",
    "        if '<EOS>' in output_list:\n",
    "            output_list.remove('<EOS>')\n",
    "        \n",
    "        output_string = (\" \").join(output_list)\n",
    "        \n",
    "        output.write(output_string + \"\\n\")\n",
    "\n",
    "    output.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    \n",
    "    print(e)\n",
    "    with open('model_NMT_bil_' + str(portion) + '_'+str(e)+'.pickle','rb') as file:\n",
    "        model_NMT = pickle.load(file)\n",
    "        \n",
    "        write_val_eval(model_NMT, val_corpus2id_fr, val_corpus2id_en, str(portion), str(e), 'bil')\n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "import string\n",
    "puncs = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training sets\n",
    "with open('tokenized_lower.en') as f:\n",
    "    train_en = [l.strip() for l in f.readlines()]\n",
    "with open('tokenized_lower.fr') as f:\n",
    "    train_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "# #validation sets\n",
    "# with open('val.en') as f:\n",
    "#     val_en = [l.strip() for l in f.readlines()]\n",
    "# with open('val.fr') as f:\n",
    "#     val_fr = [l.strip() for l in f.readlines()]\n",
    "\n",
    "# #test sets\n",
    "# with open('test_2017_flickr.en') as f:\n",
    "#     test_en = [l.strip() for l in f.readlines()]\n",
    "# with open('test_2017_flickr.fr') as f:\n",
    "#     test_fr = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 0 PAD - padding 0 for convenience in masking?\n",
    "# 1 BOS - beginning of sentence\n",
    "# 2 EOS - end of sentence\n",
    "# 3 UNK - unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_sentences(sentences):\n",
    "    tokens_list = []\n",
    "    sentence_list = []\n",
    "    for s in sentences:\n",
    "        split_sent = s.split()\n",
    "        sentence = []\n",
    "        for w in split_sent:\n",
    "\n",
    "            if w not in puncs:\n",
    "                tokens_list.append(w)\n",
    "                sentence.append(w)\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "    \n",
    "    return tokens_list, sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size EN 10203\n",
      "Vocabulary size FR 11215\n"
     ]
    }
   ],
   "source": [
    "tokens_list_en, sentence_list_en = tokens_sentences(train_en)\n",
    "\n",
    "tokens_train_en = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_en.extend(list(sorted(set(tokens_list_en))))\n",
    "vocab_size_en = len(tokens_train_en)\n",
    "print('Vocabulary size EN', vocab_size_en)\n",
    "\n",
    "count_tokens_train_en = Counter(tokens_list_en)\n",
    "\n",
    "tokens_list_fr, sentence_list_fr = tokens_sentences(train_fr)\n",
    "\n",
    "tokens_train_fr = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "tokens_train_fr.extend(list(sorted(set(tokens_list_fr))))\n",
    "vocab_size_fr = len(tokens_train_fr)\n",
    "print('Vocabulary size FR', len(tokens_train_fr))\n",
    "\n",
    "count_tokens_train_fr = Counter(tokens_list_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_dicts(tokens):\n",
    "    #default dictionary key:id value:token\n",
    "    id2tokens = defaultdict(str)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        id2tokens[i] = tokens[i]\n",
    "\n",
    "    #default dictionary key:token value:id\n",
    "    tokens2id = defaultdict(int)\n",
    "\n",
    "    for ind in id2tokens:\n",
    "        tokens2id[id2tokens[ind]] = ind\n",
    "\n",
    "    return tokens2id, id2tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10203\n",
      "11215\n"
     ]
    }
   ],
   "source": [
    "tokens2id_en, id2tokens_en = get_id_dicts(tokens_train_en)\n",
    "\n",
    "vocabulary_size_train_en = len(tokens2id_en)\n",
    "print(vocabulary_size_train_en)\n",
    "\n",
    "tokens2id_fr, id2tokens_fr = get_id_dicts(tokens_train_fr)\n",
    "\n",
    "vocabulary_size_train_fr = len(tokens2id_fr)\n",
    "print(vocabulary_size_train_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_corpus2id(sentence_list, tokens2id, max_sentence_length):\n",
    "    \n",
    "    #convert dataset to ids\n",
    "    corpus2id = []\n",
    "    \n",
    "    for s in sentence_list:\n",
    "    \n",
    "        sentence2id = []\n",
    "        sentence2id.append(tokens2id['<SOS>'])\n",
    "    \n",
    "        for w in s:\n",
    "            word_id = tokens2id[w]\n",
    "            sentence2id.append(word_id)\n",
    "        \n",
    "        \n",
    "        sentence2id.append(tokens2id['<EOS>'])\n",
    "        \n",
    "        if len(sentence2id) < max_sentence_length:\n",
    "            corpus2id.append(sentence2id)\n",
    "    \n",
    "    return corpus2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus2id_en = convert_corpus2id(sentence_list_en, tokens2id_en, max_sentence_length)\n",
    "corpus2id_fr = convert_corpus2id(sentence_list_fr, tokens2id_fr, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9496, 10178, 9971, 5319, 420, 6111, 5822, 5353, 1321, 2]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2id_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.w_embedding_dim = w_embedding_dim\n",
    "        self.p_embedding_dim = p_embedding_dim\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        #encoder\n",
    "        self.w_embeddings = nn.Embedding(self.vocab_size_fr, self.w_embedding_dim)\n",
    "        self.p_embeddings = nn.Embedding(self.max_sentence_length, self.p_embedding_dim)\n",
    "        \n",
    "        self.context_emb_dim = self.w_embedding_dim + self.p_embedding_dim\n",
    "        \n",
    "        self.average_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        \n",
    "        self.attention_projection = nn.Linear(self.context_emb_dim, self.dec_embedding_dim)\n",
    "        #do we use non-linearity after attention\n",
    "        \n",
    "        #TODO: DROPOUT\n",
    "        \n",
    "        \n",
    "    def forward(self, sent_fr, pos_fr):\n",
    "        \n",
    "        #embedded = self.embedding(input).view(1, 1, -1)\n",
    "        #TODO:BATCH\n",
    "        \n",
    "        ws = []\n",
    "        ps = []\n",
    "        es = []\n",
    "        \n",
    "        for s in range(len(sent_fr)):\n",
    "            word = sent_fr[s]\n",
    "            pos = pos_fr[s]\n",
    "            \n",
    "            w_out = self.w_embeddings(word)\n",
    "\n",
    "            p_out = self.p_embeddings(pos)\n",
    "\n",
    "            e_out = torch.cat((w_out, p_out), 0)\n",
    "    \n",
    "            ws.append(w_out)\n",
    "            ps.append(p_out)\n",
    "            es.append(e_out)\n",
    "        \n",
    "        stacked_contexts = torch.stack(es, dim = 0)\n",
    "        average_context = torch.mean(stacked_contexts, dim = 0)\n",
    "        average_context = self.average_projection(average_context)\n",
    "        \n",
    "        stacked_contexts = self.attention_projection(stacked_contexts)\n",
    "            \n",
    "        return average_context, stacked_contexts\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "        self.dec_embedding_dim = dec_embedding_dim\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size_en, self.dec_embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(self.dec_embedding_dim, self.dec_embedding_dim)\n",
    "        #self.bidirLSTM = nn.LSTM(self.embedding_dim, self.embedding_dim, bidirectional=True)\n",
    "        #TODO: LSTM, GRU \n",
    "       \n",
    "        self.pre_rnn_affine = nn.Linear(self.dec_embedding_dim*2, self.dec_embedding_dim)\n",
    "        #a linear layer after this before softmax\n",
    "        self.out_affine = nn.Linear(self.dec_embedding_dim, self.vocab_size_en)\n",
    "               \n",
    "    \n",
    "    def forward(self, gold_target_sent, encoder_avg_context, encoder_stacked_contexts, train):\n",
    "        \n",
    "        pred = []\n",
    "        attentions = []\n",
    "        \n",
    "        if train: #if training time\n",
    "            for s in range(len(gold_target_sent)):\n",
    "                gold_word = gold_target_sent[s]\n",
    "\n",
    "                output = self.embedding(gold_word)\n",
    "\n",
    "                output = self.dropout(output)\n",
    "\n",
    "                if s == 0:\n",
    "\n",
    "                    weighted_context = torch.zeros(output.shape)\n",
    "                    output = torch.cat((output, weighted_context), 0)\n",
    "\n",
    "                    output = F.relu(self.pre_rnn_affine(output))\n",
    "                    #TODO: start with 0 vector as h0\n",
    "\n",
    "                    output, hidden = self.rnn(output.view(1, 1, -1), encoder_avg_context.view(1, 1, -1))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    s_output = self.out_affine(output[0])\n",
    "                    s_output = F.log_softmax(s_output, dim=1) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "\n",
    "                    pred.append(s_output)\n",
    "\n",
    "                elif s == len(gold_target_sent)-1:\n",
    "\n",
    "                    #end of sentence\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #start with weighted context\n",
    "                    output = torch.cat((output, weighted_context), 0)\n",
    "\n",
    "                    output = F.relu(self.pre_rnn_affine(output))\n",
    "\n",
    "                    output, hidden = self.rnn(output.view(1, 1, -1), prev_hidden.view(1, 1, -1))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    s_output = self.out_affine(output[0])\n",
    "\n",
    "                    s_output = F.log_softmax(s_output, dim=1) #TODO: CHECK DIM AND OUTPUT[0]\n",
    "                    pred.append(s_output)\n",
    "\n",
    "                attention_weights_word = F.log_softmax(torch.matmul(encoder_stacked_contexts, prev_hidden.view(-1,1)), dim = 0)\n",
    "\n",
    "                #print(attention_weights_word)\n",
    "\n",
    "                weighted_context = torch.sum(torch.mul(attention_weights_word, encoder_stacked_contexts), dim = 0)\n",
    "\n",
    "                attentions.append(attention_weights_word)\n",
    "\n",
    "\n",
    "            attention_weights = torch.stack(attentions, dim=0)\n",
    "\n",
    "            pred = torch.stack(pred, dim=0)\n",
    "            \n",
    "            return pred, attention_weights\n",
    "        \n",
    "        else: #if testing time\n",
    "            \n",
    "            decoder_outputs = []\n",
    "            decoder_attentions = []\n",
    "        \n",
    "            test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "            \n",
    "            test_word_id = tokens2id_en['<SOS>']\n",
    "            \n",
    "            for w in range(self.max_sentence_length):\n",
    "       \n",
    "                if test_word_id == tokens2id_en['<EOS>']:\n",
    "                    \n",
    "                    break  \n",
    "                    \n",
    "                output = self.embedding(test_word)\n",
    "            \n",
    "                if w == 0:\n",
    "                    \n",
    "                    weighted_context = torch.zeros(output.shape)\n",
    "                    output = torch.cat((output.squeeze(), weighted_context.squeeze()), 0)\n",
    "                    \n",
    "                    output = F.relu(self.pre_rnn_affine(output))\n",
    "                    #TODO: start with 0 vector as h0\n",
    "\n",
    "                    output, hidden = self.rnn(output.view(1, 1, -1), encoder_avg_context.view(1, 1, -1))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    s_output = self.out_affine(output[0])\n",
    "                    s_output = F.log_softmax(s_output, dim=1) \n",
    "\n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "           \n",
    "                    \n",
    "                else:   \n",
    "                    #start with weighted context\n",
    "                    \n",
    "                    output = torch.cat((output.squeeze(), weighted_context.squeeze()), 0)\n",
    "\n",
    "                    output = F.relu(self.pre_rnn_affine(output))\n",
    "\n",
    "                    output, hidden = self.rnn(output.view(1, 1, -1), prev_hidden.view(1, 1, -1))\n",
    "                    prev_hidden = hidden\n",
    "\n",
    "                    s_output = self.out_affine(output[0])\n",
    "\n",
    "                    s_output = F.log_softmax(s_output, dim=1)\n",
    "                    \n",
    "                    test_word_id = int(torch.argmax(s_output))\n",
    "                    \n",
    "                    test_word = torch.tensor(np.asarray([test_word_id]), dtype = torch.long)\n",
    "                    \n",
    "                \n",
    "                attention_weights_word = F.log_softmax(torch.matmul(encoder_stacked_contexts, prev_hidden.view(-1,1)), dim = 0)\n",
    "\n",
    "                weighted_context = torch.sum(torch.mul(attention_weights_word, encoder_stacked_contexts), dim = 0)\n",
    "\n",
    "                attentions.append(attention_weights_word)\n",
    "                \n",
    "                decoder_outputs.append(test_word_id)\n",
    "                \n",
    "                                  \n",
    "\n",
    "            attention_weights = torch.stack(attentions, dim=0)            \n",
    "            \n",
    "            return decoder_outputs, attention_weights\n",
    "\n",
    "    \n",
    "    #https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, total loss, duration\n",
      "0 90.75369071960449 0:00:00.605966\n",
      "1 87.48334503173828 0:00:00.578439\n",
      "2 84.11409664154053 0:00:00.580589\n",
      "3 81.71895456314087 0:00:00.586815\n",
      "4 78.93817663192749 0:00:00.594281\n",
      "5 76.55218505859375 0:00:00.588490\n",
      "6 74.29257726669312 0:00:00.577081\n",
      "7 72.24408054351807 0:00:00.597675\n",
      "8 70.41592597961426 0:00:00.590078\n",
      "9 68.8714952468872 0:00:00.655750\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5ec682160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '<EOS>']\n",
      "['<SOS>', 'two', 'young', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '<EOS>']\n",
      "['a', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "w_embedding_dim = 100\n",
    "p_embedding_dim = 100\n",
    "dec_embedding_dim = 100\n",
    "dropout_prob = 0.1\n",
    "\n",
    "model_encoder = Encoder(vocab_size_fr, w_embedding_dim, p_embedding_dim, dec_embedding_dim, max_sentence_length)\n",
    "model_decoder = Decoder(dec_embedding_dim, vocab_size_en, max_sentence_length, dropout_prob)\n",
    "\n",
    "optimizer_encoder = optim.SGD(model_encoder.parameters(), lr = learning_rate)\n",
    "optimizer_decoder = optim.SGD(model_decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "losses = []\n",
    "avg_losses = []\n",
    "\n",
    "portion = 10\n",
    "\n",
    "train = True\n",
    "print('epoch, total loss, duration')\n",
    "for e in range(epochs):\n",
    "    \n",
    "    then = datetime.now()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for s in range(portion):\n",
    "     \n",
    "        current_input = corpus2id_fr[s]\n",
    "        gold_output = corpus2id_en[s]\n",
    "        \n",
    "        if len(current_input) > 0 and len(gold_output) > 0:\n",
    "            \n",
    "            optimizer_encoder.zero_grad()\n",
    "            optimizer_decoder.zero_grad()\n",
    "            \n",
    "            sent_fr = torch.tensor(np.asarray(current_input), dtype= torch.long)\n",
    "            sent_en = torch.tensor(np.asarray(gold_output), dtype= torch.long)\n",
    "\n",
    "            pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "            pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "            average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "            pred, attention_weights = model_decoder(sent_en, average_context, stacked_contexts, train)\n",
    "            \n",
    "            #print(pred, sent_en)\n",
    "            sent_en = sent_en[1:len(sent_en)] #skip SOS\n",
    "            loss = loss_func(pred.squeeze(), sent_en)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer_encoder.step()\n",
    "            optimizer_decoder.step()\n",
    "\n",
    "            total_loss += loss.item() \n",
    "        \n",
    "    \n",
    "    now = datetime.now()\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    print(e, total_loss, now-then)\n",
    "    \n",
    "\n",
    "with open('model_encoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_encoder,file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','wb') as file:\n",
    "    pickle.dump(model_decoder,file)\n",
    "    \n",
    "iteration= list(range(len(losses)))\n",
    "\n",
    "plt.plot(iteration, losses)\n",
    "plt.xlabel(\"Iterations for MT\")\n",
    "plt.ylabel('Average loss')\n",
    "plt.title('Evolution of the loss as a function of the iteration')\n",
    "plt.savefig(\"mt\" + str(portion)+\".png\")\n",
    "plt.show()\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[0]\n",
    "test_en_sentence = corpus2id_en[0]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_test(model_encoder, model_decoder,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "print(word_ids2string(test_fr_sentence, id2tokens_fr))\n",
    "print(word_ids2string(test_en_sentence, id2tokens_en))\n",
    "print(word_ids2string(decoder_outputs, id2tokens_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'une', 'classe', 'de', 'ballet', 'composée', 'de', 'cinq', 'filles', 'sautent', 'en', 'cadence', '<EOS>']\n",
      "['<SOS>', 'a', 'ballet', 'class', 'of', 'five', 'girls', 'jumping', 'in', 'sequence', '<EOS>']\n",
      "['a', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "pair = 10\n",
    "\n",
    "test_fr_sentence = corpus2id_fr[pair]\n",
    "test_en_sentence = corpus2id_en[pair]\n",
    "    \n",
    "decoder_outputs, decoder_attentions = evaluate_test(model_encoder, model_decoder,test_fr_sentence, test_en_sentence)\n",
    "\n",
    "print(word_ids2string(test_fr_sentence, id2tokens_fr))\n",
    "print(word_ids2string(test_en_sentence, id2tokens_en))\n",
    "print(word_ids2string(decoder_outputs, id2tokens_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('model_encoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_encoder = pickle.load(file)\n",
    "      \n",
    "\n",
    "with open('model_decoder' + str(portion) + '.pickle','rb') as file:\n",
    "    model_decoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_test(model_encoder, model_decoder, sent_fr, sent_en):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        sent_fr = torch.tensor(np.asarray(sent_fr), dtype= torch.long)\n",
    "        sent_en = torch.tensor(np.asarray(sent_en), dtype= torch.long)\n",
    "\n",
    "        pos_fr = torch.tensor(np.asarray([p for p in range(len(sent_fr))]))\n",
    "        pos_en = torch.tensor(np.asarray([p for p in range(len(sent_en))]))\n",
    "\n",
    "        average_context, stacked_contexts = model_encoder(sent_fr, pos_fr)\n",
    "        \n",
    "        decoder_outputs, decoder_attentions = model_decoder(sent_en, average_context, stacked_contexts, train=False)\n",
    "    \n",
    "    return decoder_outputs, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_ids2string(sentence, id2token):\n",
    "    \n",
    "    converted = []\n",
    "\n",
    "    for s in sentence:\n",
    "        converted.append(id2token[s])\n",
    "        \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#BEAM SEARCH\n",
    "#teacher forcing prob\n",
    "#dropout prob\n",
    "#gru lstm rnn check\n",
    "#relu before rnn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_attention(model, sentence):\n",
    "    \n",
    "    #model_encoder, model_decoder, sent_en, sent_fr\n",
    "    \n",
    "#************************************************************************\n",
    "# A is the attention torch Tensor: the output of your model\n",
    "# S is the softmax version of S, also a torch Tensor! (actually more acurately it's a Variable(Tensor(..))\n",
    "#************************************************************************\n",
    "\n",
    "    # Plot the attention tensor\n",
    "    plt.clf()\n",
    "    numpy_S = S.data.numpy() # get the data in Variable, and then the torch Tensor as numpy array\n",
    "    plt.imshow(numpy_S)\n",
    "    plt.savefig(\"attention-sent-{}-epoch-{}\".format(i, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor(np.asarray([i for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.4586, -8.4586, -7.4586, -6.4586, -5.4586, -4.4586, -3.4586,\n",
      "        -2.4586, -1.4586, -0.4586])\n"
     ]
    }
   ],
   "source": [
    "asf = F.log_softmax(a, dim=0)\n",
    "print(asf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,\n",
       "         0.7000,  0.8000,  0.9000])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.tensor(np.asarray([i+1 for i in range(10)]), dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n",
      "        [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]])\n"
     ]
    }
   ],
   "source": [
    "st = torch.stack([a,b], dim = 0)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,  0.6000,\n",
       "          0.7000,  0.8000,  0.9000],\n",
       "        [ 0.2000,  0.4000,  0.6000,  0.8000,  1.0000,  1.2000,  1.4000,\n",
       "          1.6000,  1.8000,  2.0000]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor(np.asarray([0.1, 0.2]), dtype = torch.float).view(-1,1)\n",
    "print(weights.shape)\n",
    "\n",
    "torch.mul(weights, st)\n",
    "# torch.matmul(weights.view(1,2), st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.],\n",
       "        [  1.,   2.],\n",
       "        [  2.,   3.],\n",
       "        [  3.,   4.],\n",
       "        [  4.,   5.],\n",
       "        [  5.,   6.],\n",
       "        [  6.,   7.],\n",
       "        [  7.,   8.],\n",
       "        [  8.,   9.],\n",
       "        [  9.,  10.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a,b], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.5000,  5.5000])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000,  1.5000,  2.5000,  3.5000,  4.5000,  5.5000,  6.5000,\n",
       "         7.5000,  8.5000,  9.5000])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,  0.2689,\n",
       "          0.2689,  0.2689,  0.2689],\n",
       "        [ 0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,  0.7311,\n",
       "          0.7311,  0.7311,  0.7311]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(st, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-521c823e1b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'long' is not defined"
     ]
    }
   ],
   "source": [
    "long(torch.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "test_word = torch.tensor(np.asarray([tokens2id_en['<SOS>']]), dtype = torch.long)\n",
    "print(test_word, test_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#     attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "           \n",
    "#         atts= torch.matmul(es, hidden_from_decoder)\n",
    "        \n",
    "#         weighted_context = es*attention_weights\n",
    "        \n",
    "        #if EOS for encoder, move on to the decoder\n",
    "        \n",
    "        #attention_matrices = self.attention_projection(e_out)\n",
    "        \n",
    "        #input embedding\n",
    "        #set hidden at the beginning\n",
    "        #get rnn output\n",
    "        #apply softmax\n",
    "\n",
    "        #feed actual word for training\n",
    "        #feed previous word for testing\n",
    "\n",
    "#             #view_shape = embeddings.shape[0]\n",
    "#             output, (hidden, cell) = self.bidirLSTM(embeddings.view(1, 1, -1)) \n",
    "\n",
    "#             hid_f = hidden[0]\n",
    "#             hid_b = hidden[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
